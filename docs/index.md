#ğŸ‘‹ğŸ» Welcome
OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.

<div class="grid cards" markdown>

- ğŸ“”  [__OpenPO Notebooks__](notebook.md)

</div>

## Key Features

- ğŸ”Œ **Multiple LLM Support**: Call 200+ models from HuggingFace and OpenRouter

- ğŸ§ª **Research-Backed Methodologies**: Implementation of methodologies for data synthesis from latest research papers.

- ğŸ¤ **OpenAI API Compatibility**: Support for OpenAI API format

- ğŸ’¾ **Flexible Storage:** Out of the box storage providers for HuggingFace and S3.

## How It Works
1. Collect responses from multiple models on HuggingFace or OpenRouter.
2. Run various evaluation methods on the response data.
3. Dataset is now ready for fine-tuning!



## Why Synthetic Datasets?
The cornerstone of AI excellence is data quality - a principle often expressed as "garbage in, garbage out." However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.

[Recent researches](paper.md) has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on human annotated data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data.

OpenPO aims to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.

