#ğŸ‘‹ğŸ» Welcome
OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.

<div class="grid cards" markdown>

- ğŸ“”  [__OpenPO Notebooks__](notebook.md)

</div>

## Key Features

- ğŸ¤– **Multiple LLM Support**: Collect diverse set of outputs from 200+ LLMs

- âš¡ **High Performance Inference**: Native vLLM support for optimized inference

- ğŸš€ **Scalable Processing**: Built-in batch processing capabilities for efficient large-scale data generation

- ğŸ“Š **Research-Backed Evaluation Methods**: Support for state-of-art evaluation methods for data synthesis

- ğŸ’¾ **Flexible Storage:** Out of the box storage providers for HuggingFace and S3.


## How It Works
1. Generate responses from various models on HuggingFace and OpenRouter.
2. Run evaluations on the response dataset.
3. Store, publish and fine-tune your model using the synthesized dataset.


## Why Synthetic Datasets?
The cornerstone of AI excellence is data quality - a principle often expressed as "garbage in, garbage out." However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.

[Recent researches](paper.md) has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on human annotated data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data.

OpenPO aims to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.
