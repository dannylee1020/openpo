#👋🏻 Welcome
OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.

<div class="grid cards" markdown>

- 📔  [__OpenPO Notebooks__](notebook.md)

</div>

## Key Features

- 🔌 **Multiple LLM Support**: Call 200+ models from HuggingFace and OpenRouter

- 🧪 **Research-Backed Methodologies**: Implementation of methodologies for data synthesis from latest research papers.

- 🤝 **OpenAI API Compatibility**: Support for OpenAI API format

- 💾 **Flexible Storage:** Out of the box storage providers for HuggingFace and S3.

## How It Works
1. Collect responses from multiple models on HuggingFace or OpenRouter.
2. Run various evaluation methods on the response data.
3. Dataset is now ready for fine-tuning!



## Why Synthetic Datasets?
The cornerstone of AI excellence is data quality - a principle often expressed as "garbage in, garbage out." However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.

[Recent researches](paper.md) has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on human annotated data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data.

OpenPO aims to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.

