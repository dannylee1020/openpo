#👋🏻 Welcome
OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.

<div class="grid cards" markdown>

- 📔  [__OpenPO Notebooks__](notebook.md)

</div>

## Key Features

- 🤖 **Multiple LLM Support**: Collect diverse set of outputs from 200+ LLMs

- ⚡ **High Performance Inference**: Native vLLM support for optimized inference

- 🚀 **Scalable Processing**: Built-in batch processing capabilities for efficient large-scale data generation

- 📊 **Research-Backed Evaluation Methods**: Support for state-of-art evaluation methods for data synthesis

- 💾 **Flexible Storage:** Out of the box storage providers for HuggingFace and S3.


## How It Works
1. Generate responses from various models on HuggingFace and OpenRouter.
2. Run evaluations on the response dataset.
3. Store, publish and fine-tune your model using the synthesized dataset.


## Why Synthetic Datasets?
The cornerstone of AI excellence is data quality - a principle often expressed as "garbage in, garbage out." However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.

[Recent researches](paper.md) has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on human annotated data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data.

OpenPO aims to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.
