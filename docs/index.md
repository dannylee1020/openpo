#ğŸ‘‹ğŸ» Welcome
OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.

<div class="grid cards" markdown>

- ğŸ“”  [__OpenPO Notebooks__](notebook.md)

</div>

## Key Features

- ğŸ¤– **Multiple LLM Support**: Collect diverse set of outputs from 200+ LLMs

- ğŸ“Š **Research-Backed Evaluation Methods**: Support for state-of-art evaluation methods for data synthesis

- ğŸ’¾ **Flexible Storage:** Out of the box storage providers for HuggingFace and S3.

## How It Works
1. Collect responses from multiple models on HuggingFace or OpenRouter.
2. Run out-of-the-box evaluation methods on the response data.
3. Dataset is now ready for fine-tuning!


## Why Synthetic Datasets?
The cornerstone of AI excellence is data quality - a principle often expressed as "garbage in, garbage out." However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.

[Recent researches](paper.md) has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on human annotated data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data.

OpenPO aims to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.

