#ğŸ‘‹ğŸ» Welcome
OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.

## Key Features

- ğŸ”Œ **Multiple LLM Support**: Call 200+ models from HuggingFace and OpenRouter

- ğŸ§ª **Research-Backed Methodologies**: Implementation of various methodologies on data synthesis from latest research papers. (feature coming soon!)

- ğŸ¤ **OpenAI API Compatibility**: Fully support OpenAI API format

- ğŸ’¾ **Flexible Storage:** Out of the box storage providers for Hugging Face and S3.

## How It Works
1. Collect responses from multiple models on Hugging Face and OpenRouter.
2. Run various data synthesis techniques on the outputs data. (feature coming soon!)
3. Dataset is now ready for fine-tuning!


## What is Preference Optimization?
Preference Optimization is a method to improve AI models based on human feedback about which outputs are better. Think of it as having a cooking show where judges taste two dishes and pick the better one - over time, the chef (AI model) learns what people prefer and gets better at cooking. By collecting preference data from humans and fine-tuning the model, models become better at capturing nuanced judgements and better align its output with what humans find desirable.


## Why Synthetic Datasets?
The cornerstone of AI excellence is data quality - a principle often expressed as "garbage in, garbage out." However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.

Recent research has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on human annotated data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data. Our mission is to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.

