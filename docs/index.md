#ğŸ‘‹ğŸ» Welcome
OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.

## Key Features

- ğŸ”Œ **Multiple LLM Support**: Call 200+ models from HuggingFace and OpenRouter

- ğŸ¤ **OpenAI API Compatibility**: Fully support OpenAI API format

- ğŸ’¾ **Flexible Storage:** Out of the box storage providers for Hugging Face and S3.

- ğŸ¯ **Fine-tuning Ready**: Structured data output ready for immediate model fine-tuning and preference optimization


## How It Works
1. Collect outputs from any model on Hugging Face and OpenRouter.
2. Run various ranking techniques from verified research papers on the outputs data.
3. Preference dataset is now ready for fine-tuning your model!


## What is Preference Optimization?
Preference Optimization is a method to improve AI models based on human feedback about which outputs are better. Think of it as having a cooking show where judges taste two dishes and pick the better one - over time, the chef (AI model) learns what people prefer and gets better at cooking. By collecting preference data from humans and fine-tuning the model, models become better at capturing nuanced judgements and better align its output with what humans find desirable.


## Why Synthetic Datasets?
The cornerstone of AI excellence is data quality - a principle often expressed as "garbage in, garbage out." However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.

Recent research has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on manually collected data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data. Our mission is to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.

