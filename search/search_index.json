{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b\ud83c\udffb Welcome","text":"<p>OpenPO simplifies building synthetic datasets by leveraging AI feedback from 200+ LLMs.</p> <ul> <li>\ud83d\udcd4  OpenPO Notebooks</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p>\ud83e\udd16 Multiple LLM Support: Collect diverse set of outputs from 200+ LLMs</p> </li> <li> <p>\u26a1 High Performance Inference: Native vLLM support for optimized inference</p> </li> <li> <p>\ud83d\ude80 Scalable Processing: Built-in batch processing capabilities for efficient large-scale data generation</p> </li> <li> <p>\ud83d\udcca Research-Backed Evaluation Methods: Support for state-of-art evaluation methods for data synthesis</p> </li> <li> <p>\ud83d\udcbe Flexible Storage: Out of the box storage providers for HuggingFace and S3.</p> </li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<ol> <li>Generate responses from various models on HuggingFace and OpenRouter.</li> <li>Run evaluations on the response dataset.</li> <li>Store, publish and fine-tune your model using the synthesized dataset.</li> </ol>"},{"location":"#why-synthetic-datasets","title":"Why Synthetic Datasets?","text":"<p>The cornerstone of AI excellence is data quality - a principle often expressed as \"garbage in, garbage out.\" However, obtaining high-quality training data remains one of the most significant bottlenecks in AI development, demanding substantial time and resources from teams.</p> <p>Recent researches has demonstrated breakthrough results using synthetic datasets, challenging the traditional reliance on human annotated data. OpenPO empowers developers to harness this potential by streamlining the synthesis of high-quality training data.</p> <p>OpenPO aims to eliminate the data preparation bottleneck, allowing developers to focus on what matters most: building exceptional AI applications.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#client","title":"Client","text":"<p>The main client interface for interacting with LLMs.</p>"},{"location":"api/#openpo.client.OpenPO","title":"OpenPO","text":"<p>Main client class for interacting with various LLM providers.</p> <p>This class serves as the primary interface for making completion requests to different language model providers. OpenPO takes optional api_key arguments for initialization.</p> Source code in <code>openpo/client.py</code> <pre><code>class OpenPO:\n    \"\"\"\n    Main client class for interacting with various LLM providers.\n\n    This class serves as the primary interface for making completion requests to different\n    language model providers. OpenPO takes optional api_key arguments for initialization.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        hf_api_key: Optional[str] = None,\n        openrouter_api_key: Optional[str] = None,\n        openai_api_key: Optional[str] = None,\n        anthropic_api_key: Optional[str] = None,\n    ):\n        self.hf_api_key = hf_api_key or os.getenv(\"HF_API_KEY\")\n        self.openrouter_api_key = openrouter_api_key or os.getenv(\"OPENROUTER_API_KEY\")\n        self.openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n        self.anthropic_api_key = anthropic_api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n\n        self._completion = Completion(self)\n        self._eval = Evaluation(self)\n        self._batch = Batch(self)\n\n    def _get_model_provider(self, model: str) -&gt; str:\n        try:\n            return model.split(\"/\")[0]\n        except IndexError:\n            raise ValueError(\"Invalid model format. Expected format: provider/model-id\")\n\n    def _get_model_id(self, model: str) -&gt; str:\n        try:\n            return model.split(\"/\", 1)[1]\n        except IndexError:\n            raise ValueError(\"Invalid model format. Expected format: provider/model-id\")\n\n    def _get_provider_instance(self, provider: str):\n        if provider == \"huggingface\":\n            if not self.hf_api_key:\n                raise AuthenticationError(\"HuggingFace\")\n\n            return HuggingFace(api_key=self.hf_api_key)\n\n        if provider == \"openrouter\":\n            if not self.openrouter_api_key:\n                raise AuthenticationError(\"OpenRouter\")\n\n            return OpenRouter(api_key=self.openrouter_api_key)\n\n        if provider == \"openai\":\n            if not self.openai_api_key:\n                raise AuthenticationError(\"OpenAI\")\n\n            return OpenAI(api_key=self.openai_api_key)\n\n        if provider == \"anthropic\":\n            if not self.anthropic_api_key:\n                raise AuthenticationError(\"Anthropic\")\n\n            return Anthropic(api_key=self.anthropic_api_key)\n\n        raise ProviderError(provider, \"Unsupported model provider\")\n\n    @property\n    def completion(self):\n        \"\"\"Access the chat completion functionality for LLM response.\u00b5\n        This property provides access to completion interfacce.\n\n        Returns:\n            Completion: An instance of the Completion class that provides method\n                        for generatng response from LLM.\n        \"\"\"\n        return self._completion\n\n    @property\n    def evaluate(self):\n        \"\"\"Access the evaluation functionality for LLM responses.\n        This property provides access to the evaluation interface.\n\n        Returns:\n            Evaluation: An instance of the Evaluation class that provides methods\n                       for evaluating and comparing LLM outputs.\n        \"\"\"\n        return self._eval\n\n    @property\n    def batch(self):\n        \"\"\"Access the batch processing functionality for LLM operations.\n        This property provides access to the batch processing interface\n\n        Returns:\n            Batch: An instance of the Batch class that provides methods for\n                  processing multiple LLM requests efficiently.\n        \"\"\"\n        return self._batch\n</code></pre>"},{"location":"api/#openpo.client.OpenPO.completion","title":"completion  <code>property</code>","text":"<pre><code>completion\n</code></pre> <p>Access the chat completion functionality for LLM response.\u00b5 This property provides access to completion interfacce.</p> <p>Returns:</p> Name Type Description <code>Completion</code> <p>An instance of the Completion class that provides method         for generatng response from LLM.</p>"},{"location":"api/#openpo.client.OpenPO.evaluate","title":"evaluate  <code>property</code>","text":"<pre><code>evaluate\n</code></pre> <p>Access the evaluation functionality for LLM responses. This property provides access to the evaluation interface.</p> <p>Returns:</p> Name Type Description <code>Evaluation</code> <p>An instance of the Evaluation class that provides methods        for evaluating and comparing LLM outputs.</p>"},{"location":"api/#openpo.client.OpenPO.batch","title":"batch  <code>property</code>","text":"<pre><code>batch\n</code></pre> <p>Access the batch processing functionality for LLM operations. This property provides access to the batch processing interface</p> <p>Returns:</p> Name Type Description <code>Batch</code> <p>An instance of the Batch class that provides methods for   processing multiple LLM requests efficiently.</p>"},{"location":"api/#inference","title":"Inference","text":""},{"location":"api/#openpo.VLLM","title":"VLLM","text":"<p>VLLM provider class for high-performance inference using the vLLM engine.</p> <p>This class provides an interface to the vLLM engine for running various LLMs locally.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>An instance of vLLM's LLM class that handles the model operations.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name or path of the model to load (e.g., 'meta-llama/Llama-2-7b-chat-hf').</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to vLLM's LLM initialization. These can include parameters like tensor_parallel_size, gpu_memory_utilization, etc.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the vLLM package is not installed. The error message includes installation instructions.</p> Source code in <code>openpo/resources/provider/vllm.py</code> <pre><code>class VLLM:\n    \"\"\"VLLM provider class for high-performance inference using the vLLM engine.\n\n    This class provides an interface to the vLLM engine for running various LLMs locally.\n\n    Attributes:\n        model: An instance of vLLM's LLM class that handles the model operations.\n\n    Args:\n        model (str): The name or path of the model to load (e.g., 'meta-llama/Llama-2-7b-chat-hf').\n        **kwargs: Additional keyword arguments passed to vLLM's LLM initialization.\n            These can include parameters like tensor_parallel_size, gpu_memory_utilization, etc.\n\n    Raises:\n        ImportError: If the vLLM package is not installed. The error message includes\n            installation instructions.\n    \"\"\"\n\n    def __init__(self, model: str, **kwargs) -&gt; None:\n        try:\n            from vllm import LLM, SamplingParams\n        except ImportError:\n            raise ImportError(\n                \"vLLM requires additional dependencies. Install with: pip install openpo[eval]\"\n            )\n\n        self.model = LLM(model=model, **kwargs)\n        self.SamplingParams = SamplingParams\n\n    def generate(\n        self,\n        messages: List,\n        use_tqdm: bool = True,\n        lora_request=None,\n        chat_template=None,\n        chat_template_content_format=\"auto\",\n        add_generation_prompt: bool = True,\n        continue_final_message: bool = False,\n        tools: Optional[List[Dict[str, Any]]] = None,\n        mm_processor_kwargs: Optional[Dict[str, Any]] = None,\n        sampling_params: Optional[Dict] = {},\n    ) -&gt; List:\n        \"\"\"Generate responses using the vLLM model.\n\n        This method processes input messages and generates responses using the loaded model.\n        It supports various generation parameters and features like chat templates, LoRA\n        adapters, and tool-based interactions.\n\n        Args:\n            messages (List): List of input messages to process.\n            use_tqdm (bool, optional): Whether to show a progress bar. Defaults to True.\n            lora_request: Optional LoRA adapter configuration for on-the-fly model adaptation.\n            chat_template: Optional template for formatting chat messages.\n            chat_template_content_format (str, optional): Format for the chat template content.\n                Defaults to \"auto\".\n            add_generation_prompt (bool, optional): Whether to add generation prompt.\n                Defaults to True.\n            continue_final_message (bool, optional): Whether to continue from the final message.\n                Defaults to False.\n            tools (Optional[List[Dict[str, Any]]], optional): List of tools available for the model.\n            mm_processor_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments\n                for multimodal processing.\n            sampling_params (Optional[dict]): Model specific parameters passed to vLLM's SamplingParams.\n\n        Returns:\n            The generated vLLM output from the model.\n\n        Raises:\n            ProviderError: If generation fails, with details about the error.\n        \"\"\"\n        try:\n            params = self.SamplingParams(**sampling_params)\n            res = self.model.chat(\n                messages=messages,\n                use_tqdm=use_tqdm,\n                lora_request=lora_request,\n                chat_template=chat_template,\n                chat_template_content_format=chat_template_content_format,\n                add_generation_prompt=add_generation_prompt,\n                continue_final_message=continue_final_message,\n                tools=tools,\n                mm_processor_kwargs=mm_processor_kwargs,\n                sampling_params=params,\n            )\n\n            return res\n        except Exception as e:\n            raise ProviderError(\n                provider=\"vllm\", message=f\"model inference failed: {str(e)}\"\n            )\n</code></pre>"},{"location":"api/#openpo.VLLM.generate","title":"generate","text":"<pre><code>generate(messages: List, use_tqdm: bool = True, lora_request=None, chat_template=None, chat_template_content_format='auto', add_generation_prompt: bool = True, continue_final_message: bool = False, tools: Optional[List[Dict[str, Any]]] = None, mm_processor_kwargs: Optional[Dict[str, Any]] = None, sampling_params: Optional[Dict] = {}) -&gt; List\n</code></pre> <p>Generate responses using the vLLM model.</p> <p>This method processes input messages and generates responses using the loaded model. It supports various generation parameters and features like chat templates, LoRA adapters, and tool-based interactions.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List</code> <p>List of input messages to process.</p> required <code>use_tqdm</code> <code>bool</code> <p>Whether to show a progress bar. Defaults to True.</p> <code>True</code> <code>lora_request</code> <p>Optional LoRA adapter configuration for on-the-fly model adaptation.</p> <code>None</code> <code>chat_template</code> <p>Optional template for formatting chat messages.</p> <code>None</code> <code>chat_template_content_format</code> <code>str</code> <p>Format for the chat template content. Defaults to \"auto\".</p> <code>'auto'</code> <code>add_generation_prompt</code> <code>bool</code> <p>Whether to add generation prompt. Defaults to True.</p> <code>True</code> <code>continue_final_message</code> <code>bool</code> <p>Whether to continue from the final message. Defaults to False.</p> <code>False</code> <code>tools</code> <code>Optional[List[Dict[str, Any]]]</code> <p>List of tools available for the model.</p> <code>None</code> <code>mm_processor_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional keyword arguments for multimodal processing.</p> <code>None</code> <code>sampling_params</code> <code>Optional[dict]</code> <p>Model specific parameters passed to vLLM's SamplingParams.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List</code> <p>The generated vLLM output from the model.</p> <p>Raises:</p> Type Description <code>ProviderError</code> <p>If generation fails, with details about the error.</p> Source code in <code>openpo/resources/provider/vllm.py</code> <pre><code>def generate(\n    self,\n    messages: List,\n    use_tqdm: bool = True,\n    lora_request=None,\n    chat_template=None,\n    chat_template_content_format=\"auto\",\n    add_generation_prompt: bool = True,\n    continue_final_message: bool = False,\n    tools: Optional[List[Dict[str, Any]]] = None,\n    mm_processor_kwargs: Optional[Dict[str, Any]] = None,\n    sampling_params: Optional[Dict] = {},\n) -&gt; List:\n    \"\"\"Generate responses using the vLLM model.\n\n    This method processes input messages and generates responses using the loaded model.\n    It supports various generation parameters and features like chat templates, LoRA\n    adapters, and tool-based interactions.\n\n    Args:\n        messages (List): List of input messages to process.\n        use_tqdm (bool, optional): Whether to show a progress bar. Defaults to True.\n        lora_request: Optional LoRA adapter configuration for on-the-fly model adaptation.\n        chat_template: Optional template for formatting chat messages.\n        chat_template_content_format (str, optional): Format for the chat template content.\n            Defaults to \"auto\".\n        add_generation_prompt (bool, optional): Whether to add generation prompt.\n            Defaults to True.\n        continue_final_message (bool, optional): Whether to continue from the final message.\n            Defaults to False.\n        tools (Optional[List[Dict[str, Any]]], optional): List of tools available for the model.\n        mm_processor_kwargs (Optional[Dict[str, Any]], optional): Additional keyword arguments\n            for multimodal processing.\n        sampling_params (Optional[dict]): Model specific parameters passed to vLLM's SamplingParams.\n\n    Returns:\n        The generated vLLM output from the model.\n\n    Raises:\n        ProviderError: If generation fails, with details about the error.\n    \"\"\"\n    try:\n        params = self.SamplingParams(**sampling_params)\n        res = self.model.chat(\n            messages=messages,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            chat_template=chat_template,\n            chat_template_content_format=chat_template_content_format,\n            add_generation_prompt=add_generation_prompt,\n            continue_final_message=continue_final_message,\n            tools=tools,\n            mm_processor_kwargs=mm_processor_kwargs,\n            sampling_params=params,\n        )\n\n        return res\n    except Exception as e:\n        raise ProviderError(\n            provider=\"vllm\", message=f\"model inference failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"api/#openpo.resources.completion.completion.Completion","title":"Completion","text":"Source code in <code>openpo/resources/completion/completion.py</code> <pre><code>class Completion:\n    def __init__(self, client):\n        self.client = client\n\n    def generate(\n        self,\n        model: Union[str, List[str]],\n        messages: List[Dict[str, Any]],\n        params: Optional[Dict[str, Any]] = None,\n    ) -&gt; List[ChatCompletionOutput]:\n        \"\"\"Generate completions using the specified LLM provider.\n\n        Args:\n            model (str, List[str]): model identifier or list of model identifiers to use for generation. Follows &lt;provider&gt;/&lt;model-identifier&gt; format.\n            messages (List[Dict[str, Any]]): List of message dictionaries containing\n                the conversation history and prompts.\n            params (Optional[Dict[str, Any]]): Additional model parameters for the request (e.g., temperature, max_tokens).\n\n        Returns:\n            The response from the LLM provider containing the generated completions.\n\n        Raises:\n            AuthenticationError: If required API keys are missing or invalid.\n            ProviderError: For provider-specific errors during completion generation.\n            ValueError: If the model format is invalid.\n        \"\"\"\n        if isinstance(model, str):\n            try:\n                provider = self.client._get_model_provider(model=model)\n                model_id = self.client._get_model_id(model=model)\n                llm = self.client._get_provider_instance(provider=provider)\n\n                res = llm.generate(model=model_id, messages=messages, params=params)\n                return res\n            except Exception as e:\n                raise ProviderError(\n                    provider=provider,\n                    message=f\"Failed to execute chat completions: {str(e)}\",\n                )\n        responses = []\n        for m in model:\n            try:\n                provider = self.client._get_model_provider(model=m)\n                model_id = self.client._get_model_id(model=m)\n                llm = self.client._get_provider_instance(provider=provider)\n\n                res = llm.generate(model=model_id, messages=messages, params=params)\n                responses.append(res)\n            except (AuthenticationError, ValueError) as e:\n                # Re-raise authentication and validation errors as is\n                raise e\n            except Exception as e:\n                raise ProviderError(\n                    provider=provider,\n                    message=f\"Failed to execute chat completions: {str(e)}\",\n                )\n        return responses\n</code></pre>"},{"location":"api/#openpo.resources.completion.completion.Completion.generate","title":"generate","text":"<pre><code>generate(model: Union[str, List[str]], messages: List[Dict[str, Any]], params: Optional[Dict[str, Any]] = None) -&gt; List[ChatCompletionOutput]\n</code></pre> <p>Generate completions using the specified LLM provider.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>(str, List[str])</code> <p>model identifier or list of model identifiers to use for generation. Follows / format. required <code>messages</code> <code>List[Dict[str, Any]]</code> <p>List of message dictionaries containing the conversation history and prompts.</p> required <code>params</code> <code>Optional[Dict[str, Any]]</code> <p>Additional model parameters for the request (e.g., temperature, max_tokens).</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ChatCompletionOutput]</code> <p>The response from the LLM provider containing the generated completions.</p> <p>Raises:</p> Type Description <code>AuthenticationError</code> <p>If required API keys are missing or invalid.</p> <code>ProviderError</code> <p>For provider-specific errors during completion generation.</p> <code>ValueError</code> <p>If the model format is invalid.</p> Source code in <code>openpo/resources/completion/completion.py</code> <pre><code>def generate(\n    self,\n    model: Union[str, List[str]],\n    messages: List[Dict[str, Any]],\n    params: Optional[Dict[str, Any]] = None,\n) -&gt; List[ChatCompletionOutput]:\n    \"\"\"Generate completions using the specified LLM provider.\n\n    Args:\n        model (str, List[str]): model identifier or list of model identifiers to use for generation. Follows &lt;provider&gt;/&lt;model-identifier&gt; format.\n        messages (List[Dict[str, Any]]): List of message dictionaries containing\n            the conversation history and prompts.\n        params (Optional[Dict[str, Any]]): Additional model parameters for the request (e.g., temperature, max_tokens).\n\n    Returns:\n        The response from the LLM provider containing the generated completions.\n\n    Raises:\n        AuthenticationError: If required API keys are missing or invalid.\n        ProviderError: For provider-specific errors during completion generation.\n        ValueError: If the model format is invalid.\n    \"\"\"\n    if isinstance(model, str):\n        try:\n            provider = self.client._get_model_provider(model=model)\n            model_id = self.client._get_model_id(model=model)\n            llm = self.client._get_provider_instance(provider=provider)\n\n            res = llm.generate(model=model_id, messages=messages, params=params)\n            return res\n        except Exception as e:\n            raise ProviderError(\n                provider=provider,\n                message=f\"Failed to execute chat completions: {str(e)}\",\n            )\n    responses = []\n    for m in model:\n        try:\n            provider = self.client._get_model_provider(model=m)\n            model_id = self.client._get_model_id(model=m)\n            llm = self.client._get_provider_instance(provider=provider)\n\n            res = llm.generate(model=model_id, messages=messages, params=params)\n            responses.append(res)\n        except (AuthenticationError, ValueError) as e:\n            # Re-raise authentication and validation errors as is\n            raise e\n        except Exception as e:\n            raise ProviderError(\n                provider=provider,\n                message=f\"Failed to execute chat completions: {str(e)}\",\n            )\n    return responses\n</code></pre>"},{"location":"api/#openpo.resources.eval.eval.Evaluation","title":"Evaluation","text":"Source code in <code>openpo/resources/eval/eval.py</code> <pre><code>class Evaluation:\n    def __init__(self, client):\n        self.client = client\n\n    def _validate_provider(self, provider: str) -&gt; None:\n        if provider not in [\"openai\", \"anthropic\"]:\n            raise ProviderError(provider, \"Provider not supported for evaluation\")\n\n    def _parse_response(self, response) -&gt; List[Dict]:\n        try:\n            if \"chatcmpl\" in response.id:\n                return json.loads(response.choices[0].message.content)[\"evaluation\"]\n            return response.content[0].input[\"evaluation\"]\n        except Exception as e:\n            raise Exception(f\"Error parsing model responses: {e}\")\n\n    def eval(\n        self,\n        model: Union[str, List[str]],\n        questions: List[str],\n        responses: List[List[str]],\n        prompt: Optional[str] = None,\n    ) -&gt; List[Dict]:\n        \"\"\"Evaluate responses using either single or multiple LLMs as judges.\n\n        Args:\n            model (str, List[str]): model identifier or list of them to use as a judge. Follows provider/model-identifier format.\n            questions (List[str]): Questions for each response pair.\n            responses (List[List[str]]): Pairwise responses to evaluate.\n            prompt (str): Optional custom prompt for judge model to follow.\n\n        Returns:\n            List[Dict]: The evaluation data for responses. Response returns preferred, rejected, confidence_score and reason.\n\n        Raises:\n            AuthenticationError: If required API keys are missing or invalid.\n            ProviderError: For provider-specific errors during evaluation.\n            ValueError: If the model format is invalid or required models are missing.\n        \"\"\"\n        if isinstance(model, str):\n            try:\n                provider = self.client._get_model_provider(model)\n                model_id = self.client._get_model_id(model)\n\n                self._validate_provider(provider)\n\n                llm = self.client._get_provider_instance(provider)\n                res = llm.generate(\n                    model=model_id,\n                    questions=questions,\n                    responses=responses,\n                    prompt=prompt if prompt else None,\n                )\n                return res\n            except Exception as e:\n                raise ProviderError(\n                    provider=provider, message=f\"Error during evaluation: {str(e)}\"\n                )\n\n        eval_res = []\n        for m in model:\n            try:\n                provider = self.client._get_model_provider(m)\n                model_id = self.client._get_model_id(m)\n\n                self._validate_provider(provider)\n\n                llm = self.client._get_provider_instance(provider=provider)\n                res = llm.generate(\n                    model=model_id,\n                    questions=questions,\n                    responses=responses,\n                    prompt=prompt if prompt else None,\n                )\n                eval_res.append(res)\n            except Exception as e:\n                raise ProviderError(\n                    provider=provider, message=f\"Error during evaluation: {str(e)}\"\n                )\n        return eval_res\n\n    def get_consensus(self, eval_A: List, eval_B: List) -&gt; List:\n        \"\"\"Reach consensus between two evaluation results\n\n        Args:\n            eval_A (List): List of batch results to compare\n            eval_B (List): List of batch results to compare\n\n        Returns:\n            List: List of evaluation results where both providers agreed on the rank\n\n        Raises:\n            Exception: If there's an error processing the batch results\n        \"\"\"\n        try:\n            parsed_a = self._parse_response(\n                response=eval_A,\n            )\n            parsed_b = self._parse_response(\n                response=eval_B,\n            )\n\n            res = []\n            check = {}\n\n            for e in parsed_a:\n                q_index = e[\"q_index\"]\n                check[q_index] = e[\"rank\"]\n\n            for e in parsed_b:\n                q_index = e[\"q_index\"]\n                if q_index in check and check[q_index] == e[\"rank\"]:\n                    res.append(e)\n            return res\n        except Exception as e:\n            raise Exception(f\"Error processing responses for consensus: {str(e)}\")\n</code></pre>"},{"location":"api/#openpo.resources.eval.eval.Evaluation.eval","title":"eval","text":"<pre><code>eval(model: Union[str, List[str]], questions: List[str], responses: List[List[str]], prompt: Optional[str] = None) -&gt; List[Dict]\n</code></pre> <p>Evaluate responses using either single or multiple LLMs as judges.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>(str, List[str])</code> <p>model identifier or list of them to use as a judge. Follows provider/model-identifier format.</p> required <code>questions</code> <code>List[str]</code> <p>Questions for each response pair.</p> required <code>responses</code> <code>List[List[str]]</code> <p>Pairwise responses to evaluate.</p> required <code>prompt</code> <code>str</code> <p>Optional custom prompt for judge model to follow.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: The evaluation data for responses. Response returns preferred, rejected, confidence_score and reason.</p> <p>Raises:</p> Type Description <code>AuthenticationError</code> <p>If required API keys are missing or invalid.</p> <code>ProviderError</code> <p>For provider-specific errors during evaluation.</p> <code>ValueError</code> <p>If the model format is invalid or required models are missing.</p> Source code in <code>openpo/resources/eval/eval.py</code> <pre><code>def eval(\n    self,\n    model: Union[str, List[str]],\n    questions: List[str],\n    responses: List[List[str]],\n    prompt: Optional[str] = None,\n) -&gt; List[Dict]:\n    \"\"\"Evaluate responses using either single or multiple LLMs as judges.\n\n    Args:\n        model (str, List[str]): model identifier or list of them to use as a judge. Follows provider/model-identifier format.\n        questions (List[str]): Questions for each response pair.\n        responses (List[List[str]]): Pairwise responses to evaluate.\n        prompt (str): Optional custom prompt for judge model to follow.\n\n    Returns:\n        List[Dict]: The evaluation data for responses. Response returns preferred, rejected, confidence_score and reason.\n\n    Raises:\n        AuthenticationError: If required API keys are missing or invalid.\n        ProviderError: For provider-specific errors during evaluation.\n        ValueError: If the model format is invalid or required models are missing.\n    \"\"\"\n    if isinstance(model, str):\n        try:\n            provider = self.client._get_model_provider(model)\n            model_id = self.client._get_model_id(model)\n\n            self._validate_provider(provider)\n\n            llm = self.client._get_provider_instance(provider)\n            res = llm.generate(\n                model=model_id,\n                questions=questions,\n                responses=responses,\n                prompt=prompt if prompt else None,\n            )\n            return res\n        except Exception as e:\n            raise ProviderError(\n                provider=provider, message=f\"Error during evaluation: {str(e)}\"\n            )\n\n    eval_res = []\n    for m in model:\n        try:\n            provider = self.client._get_model_provider(m)\n            model_id = self.client._get_model_id(m)\n\n            self._validate_provider(provider)\n\n            llm = self.client._get_provider_instance(provider=provider)\n            res = llm.generate(\n                model=model_id,\n                questions=questions,\n                responses=responses,\n                prompt=prompt if prompt else None,\n            )\n            eval_res.append(res)\n        except Exception as e:\n            raise ProviderError(\n                provider=provider, message=f\"Error during evaluation: {str(e)}\"\n            )\n    return eval_res\n</code></pre>"},{"location":"api/#openpo.resources.eval.eval.Evaluation.get_consensus","title":"get_consensus","text":"<pre><code>get_consensus(eval_A: List, eval_B: List) -&gt; List\n</code></pre> <p>Reach consensus between two evaluation results</p> <p>Parameters:</p> Name Type Description Default <code>eval_A</code> <code>List</code> <p>List of batch results to compare</p> required <code>eval_B</code> <code>List</code> <p>List of batch results to compare</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>List of evaluation results where both providers agreed on the rank</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error processing the batch results</p> Source code in <code>openpo/resources/eval/eval.py</code> <pre><code>def get_consensus(self, eval_A: List, eval_B: List) -&gt; List:\n    \"\"\"Reach consensus between two evaluation results\n\n    Args:\n        eval_A (List): List of batch results to compare\n        eval_B (List): List of batch results to compare\n\n    Returns:\n        List: List of evaluation results where both providers agreed on the rank\n\n    Raises:\n        Exception: If there's an error processing the batch results\n    \"\"\"\n    try:\n        parsed_a = self._parse_response(\n            response=eval_A,\n        )\n        parsed_b = self._parse_response(\n            response=eval_B,\n        )\n\n        res = []\n        check = {}\n\n        for e in parsed_a:\n            q_index = e[\"q_index\"]\n            check[q_index] = e[\"rank\"]\n\n        for e in parsed_b:\n            q_index = e[\"q_index\"]\n            if q_index in check and check[q_index] == e[\"rank\"]:\n                res.append(e)\n        return res\n    except Exception as e:\n        raise Exception(f\"Error processing responses for consensus: {str(e)}\")\n</code></pre>"},{"location":"api/#openpo.resources.batch.batch.Batch","title":"Batch","text":"Source code in <code>openpo/resources/batch/batch.py</code> <pre><code>class Batch:\n    def __init__(self, client):\n        self.client = client\n        self._openai_client = None\n        self._anthropic_client = None\n\n    @property\n    def openai_client(self):\n        if self._openai_client is None:\n            from openai import OpenAI as OpenAIClient\n\n            if not self.client.openai_api_key:\n                raise AuthenticationError(\"OpenAI\")\n            self._openai_client = OpenAIClient(api_key=self.client.openai_api_key)\n        return self._openai_client\n\n    @property\n    def anthropic_client(self):\n        if self._anthropic_client is None:\n            from anthropic import Anthropic as AnthropicClient\n\n            if not self.client.anthropic_api_key:\n                raise AuthenticationError(\"Anthropic\")\n            self._anthropic_client = AnthropicClient(\n                api_key=self.client.anthropic_api_key\n            )\n        return self._anthropic_client\n\n    def _validate_provider(self, provider: str) -&gt; None:\n        if provider not in [\"openai\", \"anthropic\"]:\n            raise ProviderError(provider, \"Provider not supported for evaluation\")\n\n    def eval(\n        self,\n        model: Union[str, List[str]],\n        questions: List[str],\n        responses: List[List[str]],\n        prompt: Optional[str] = None,\n    ):\n        \"\"\"Use input model as a judge to evaluate responses.\n\n        Args:\n            model (str, List[str]): model identifier or list of them to use as a judge. Follows provider/model-identifier format.\n            questions (List(str)): Questions for each response pair.\n            responses (List[List[str]]): Pairwise responses to evaluate.\n            prompt (str): Optional custom prompt for judge model to follow.\n\n        Returns (Dict): The evaluation data for responses with preferred, rejected, confidence_score and reason.\n\n        Raises:\n            AuthenticationError: If required API keys are missing or invalid.\n            ProviderError: For provider-specific errors during evaluation.\n            ValueError: If the model format is invalid or provider is not supported.\n        \"\"\"\n        if isinstance(model, str):\n            try:\n                provider = self.client._get_model_provider(model)\n                model_id = self.client._get_model_id(model)\n\n                self._validate_provider(provider)\n\n                llm = self.client._get_provider_instance(provider=provider)\n                res = llm.generate_batch(\n                    model=model_id,\n                    questions=questions,\n                    responses=responses,\n                    prompt=prompt if prompt else None,\n                )\n                return res\n            except Exception as e:\n                raise ProviderError(\n                    provider=provider,\n                    message=f\"Error during batch processing: {str(e)}\",\n                )\n\n        result = []\n        for m in model:\n            try:\n                provider = self.client._get_model_provider(m)\n                model_id = self.client._get_model_id(m)\n\n                self._validate_provider(provider)\n\n                llm = self.client._get_provider_instance(provider=provider)\n                res = llm.generate_batch(\n                    model=model_id,\n                    questions=questions,\n                    responses=responses,\n                    prompt=prompt if prompt else None,\n                )\n                result.append(res)\n            except Exception as e:\n                raise ProviderError(\n                    provider=provider,\n                    message=f\"Error during batch processing: {str(e)}\",\n                )\n\n        return result\n\n    def check_status(self, batch_id: str):\n        if batch_id.split(\"_\")[0] == \"batch\":\n            status = self.openai_client.batches.retrieve(batch_id)\n        else:\n            status = self.anthropic_client.beta.messages.batches.retrieve(batch_id)\n\n        return status\n\n    def load_batch(self, filename: str, provider: str):\n        data = []\n        if provider == \"openai\":\n            res = self.openai_client.files.content(filename)\n\n            for line in res.text.splitlines():\n                if line.strip():  # Skip empty lines\n                    data.append(json.loads(line))\n\n            return data\n\n        if provider == \"anthropic\":\n            res = self.anthropic_client.beta.messages.batches.results(filename)\n            for r in res:\n                data.append(r)\n\n            return data\n\n    def get_consensus(\n        self,\n        batch_A: List,\n        batch_B: List,\n    ) -&gt; List[Dict]:\n        \"\"\"Reach consensus between two batch results.\n\n        Args:\n            batch_A (List): List of batch results to compare\n            batch_B (List): List of batch results to compare\n\n        Returns:\n            List[Dict]: List of evaluation results where both providers agree on\n\n        Raises:\n            Exception: If there's an error processing the batch results\n        \"\"\"\n        try:\n            # uses dictionary to keep record of index and rank\n            # only requires single pass on batch data to reach consensus.\n            res = []\n            check = {}\n            for r in batch_A:\n                # check if batch is from openai\n                if isinstance(r, dict):\n                    custom_id = r[\"custom_id\"]\n                    content = json.loads(\n                        r[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n                    )\n                else:\n                    custom_id = r.custom_id\n                    content = r.result.message.content[0].input\n\n                check[custom_id] = content[\"evaluation\"][0][\"rank\"]\n\n            for r in batch_B:\n                if isinstance(r, dict):\n                    custom_id = r[\"custom_id\"]\n                    content = json.loads(\n                        r[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n                    )\n                else:\n                    custom_id = r.custom_id\n                    content = r.result.message.content[0].input\n\n                if (\n                    custom_id in check\n                    and check[custom_id] == content[\"evaluation\"][0][\"rank\"]\n                ):\n                    record = {\"q_index\": custom_id} | content[\"evaluation\"][0]\n                    res.append(record)\n\n            return res\n        except Exception as e:\n            raise Exception(f\"Error processing batch results: {str(e)}\")\n</code></pre>"},{"location":"api/#openpo.resources.batch.batch.Batch.eval","title":"eval","text":"<pre><code>eval(model: Union[str, List[str]], questions: List[str], responses: List[List[str]], prompt: Optional[str] = None)\n</code></pre> <p>Use input model as a judge to evaluate responses.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>(str, List[str])</code> <p>model identifier or list of them to use as a judge. Follows provider/model-identifier format.</p> required <code>questions</code> <code>List(str</code> <p>Questions for each response pair.</p> required <code>responses</code> <code>List[List[str]]</code> <p>Pairwise responses to evaluate.</p> required <code>prompt</code> <code>str</code> <p>Optional custom prompt for judge model to follow.</p> <code>None</code> <p>Returns (Dict): The evaluation data for responses with preferred, rejected, confidence_score and reason.</p> <p>Raises:</p> Type Description <code>AuthenticationError</code> <p>If required API keys are missing or invalid.</p> <code>ProviderError</code> <p>For provider-specific errors during evaluation.</p> <code>ValueError</code> <p>If the model format is invalid or provider is not supported.</p> Source code in <code>openpo/resources/batch/batch.py</code> <pre><code>def eval(\n    self,\n    model: Union[str, List[str]],\n    questions: List[str],\n    responses: List[List[str]],\n    prompt: Optional[str] = None,\n):\n    \"\"\"Use input model as a judge to evaluate responses.\n\n    Args:\n        model (str, List[str]): model identifier or list of them to use as a judge. Follows provider/model-identifier format.\n        questions (List(str)): Questions for each response pair.\n        responses (List[List[str]]): Pairwise responses to evaluate.\n        prompt (str): Optional custom prompt for judge model to follow.\n\n    Returns (Dict): The evaluation data for responses with preferred, rejected, confidence_score and reason.\n\n    Raises:\n        AuthenticationError: If required API keys are missing or invalid.\n        ProviderError: For provider-specific errors during evaluation.\n        ValueError: If the model format is invalid or provider is not supported.\n    \"\"\"\n    if isinstance(model, str):\n        try:\n            provider = self.client._get_model_provider(model)\n            model_id = self.client._get_model_id(model)\n\n            self._validate_provider(provider)\n\n            llm = self.client._get_provider_instance(provider=provider)\n            res = llm.generate_batch(\n                model=model_id,\n                questions=questions,\n                responses=responses,\n                prompt=prompt if prompt else None,\n            )\n            return res\n        except Exception as e:\n            raise ProviderError(\n                provider=provider,\n                message=f\"Error during batch processing: {str(e)}\",\n            )\n\n    result = []\n    for m in model:\n        try:\n            provider = self.client._get_model_provider(m)\n            model_id = self.client._get_model_id(m)\n\n            self._validate_provider(provider)\n\n            llm = self.client._get_provider_instance(provider=provider)\n            res = llm.generate_batch(\n                model=model_id,\n                questions=questions,\n                responses=responses,\n                prompt=prompt if prompt else None,\n            )\n            result.append(res)\n        except Exception as e:\n            raise ProviderError(\n                provider=provider,\n                message=f\"Error during batch processing: {str(e)}\",\n            )\n\n    return result\n</code></pre>"},{"location":"api/#openpo.resources.batch.batch.Batch.get_consensus","title":"get_consensus","text":"<pre><code>get_consensus(batch_A: List, batch_B: List) -&gt; List[Dict]\n</code></pre> <p>Reach consensus between two batch results.</p> <p>Parameters:</p> Name Type Description Default <code>batch_A</code> <code>List</code> <p>List of batch results to compare</p> required <code>batch_B</code> <code>List</code> <p>List of batch results to compare</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of evaluation results where both providers agree on</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error processing the batch results</p> Source code in <code>openpo/resources/batch/batch.py</code> <pre><code>def get_consensus(\n    self,\n    batch_A: List,\n    batch_B: List,\n) -&gt; List[Dict]:\n    \"\"\"Reach consensus between two batch results.\n\n    Args:\n        batch_A (List): List of batch results to compare\n        batch_B (List): List of batch results to compare\n\n    Returns:\n        List[Dict]: List of evaluation results where both providers agree on\n\n    Raises:\n        Exception: If there's an error processing the batch results\n    \"\"\"\n    try:\n        # uses dictionary to keep record of index and rank\n        # only requires single pass on batch data to reach consensus.\n        res = []\n        check = {}\n        for r in batch_A:\n            # check if batch is from openai\n            if isinstance(r, dict):\n                custom_id = r[\"custom_id\"]\n                content = json.loads(\n                    r[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n                )\n            else:\n                custom_id = r.custom_id\n                content = r.result.message.content[0].input\n\n            check[custom_id] = content[\"evaluation\"][0][\"rank\"]\n\n        for r in batch_B:\n            if isinstance(r, dict):\n                custom_id = r[\"custom_id\"]\n                content = json.loads(\n                    r[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n                )\n            else:\n                custom_id = r.custom_id\n                content = r.result.message.content[0].input\n\n            if (\n                custom_id in check\n                and check[custom_id] == content[\"evaluation\"][0][\"rank\"]\n            ):\n                record = {\"q_index\": custom_id} | content[\"evaluation\"][0]\n                res.append(record)\n\n        return res\n    except Exception as e:\n        raise Exception(f\"Error processing batch results: {str(e)}\")\n</code></pre>"},{"location":"api/#evaluation-model","title":"Evaluation Model","text":""},{"location":"api/#openpo.resources.pairrm.pairrm.PairRM","title":"PairRM","text":"<p>A class that implements the Pairwise Rewards Model (PairRM) for evaluating and ranking LLM responses.</p> <p>This class uses the llm-blender package to load and utilize the PairRM model, which can rank multiple responses for a given prompt based on their quality.</p> Source code in <code>openpo/resources/pairrm/pairrm.py</code> <pre><code>class PairRM:\n    \"\"\"\n    A class that implements the Pairwise Rewards Model (PairRM) for evaluating and ranking LLM responses.\n\n    This class uses the llm-blender package to load and utilize the PairRM model, which can rank\n    multiple responses for a given prompt based on their quality.\n\n    \"\"\"\n\n    def __init__(self):\n        try:\n            import llm_blender\n        except ImportError:\n            raise ImportError(\n                \"PairRM requires additional dependencies. Install with: pip install openpo[eval]\"\n            )\n\n        self.blender = llm_blender.Blender()\n        self.blender.loadranker(\"llm-blender/PairRM\")\n\n    def _format_preference(\n        self,\n        ranks: List,\n        prompts: List,\n        responses: List,\n    ) -&gt; List[dict]:\n        dataset = []\n\n        for i in range(len(prompts)):\n            try:\n                dataset.append(\n                    {\n                        \"prompt\": prompts[i],\n                        \"preferred\": responses[i][np.where(ranks[i] == 1)[0][0]],\n                        \"rejected\": responses[i][\n                            np.where(ranks[i] == max(ranks[i]))[0][0]\n                        ],\n                        \"ranks\": ranks[i],\n                    }\n                )\n            except (ValueError, IndexError):\n                print(f\"Skipping index {i} due to ranking issues.\")\n                continue\n\n        return dataset\n\n    def eval(\n        self,\n        prompts: List,\n        responses: List,\n    ) -&gt; List[dict]:\n        \"\"\"\n        Evaluates and ranks multiple responses for given prompts.\n\n        Args:\n            prompts (List): List of input prompts to evaluate.\n            responses (List): List of response sets to be ranked.\n                Each response set should contain multiple responses for the corresponding prompt.\n\n        Returns:\n            List[dict]: A formatted list of preference data containing the ranking results.\n                See _format_preference method for the structure of the returned data.\n        \"\"\"\n        ranks = self.blender.rank(prompts, responses)\n        return self._format_preference(ranks, prompts, responses)\n</code></pre>"},{"location":"api/#openpo.resources.pairrm.pairrm.PairRM.eval","title":"eval","text":"<pre><code>eval(prompts: List, responses: List) -&gt; List[dict]\n</code></pre> <p>Evaluates and ranks multiple responses for given prompts.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List</code> <p>List of input prompts to evaluate.</p> required <code>responses</code> <code>List</code> <p>List of response sets to be ranked. Each response set should contain multiple responses for the corresponding prompt.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: A formatted list of preference data containing the ranking results. See _format_preference method for the structure of the returned data.</p> Source code in <code>openpo/resources/pairrm/pairrm.py</code> <pre><code>def eval(\n    self,\n    prompts: List,\n    responses: List,\n) -&gt; List[dict]:\n    \"\"\"\n    Evaluates and ranks multiple responses for given prompts.\n\n    Args:\n        prompts (List): List of input prompts to evaluate.\n        responses (List): List of response sets to be ranked.\n            Each response set should contain multiple responses for the corresponding prompt.\n\n    Returns:\n        List[dict]: A formatted list of preference data containing the ranking results.\n            See _format_preference method for the structure of the returned data.\n    \"\"\"\n    ranks = self.blender.rank(prompts, responses)\n    return self._format_preference(ranks, prompts, responses)\n</code></pre>"},{"location":"api/#openpo.resources.prometheus2.prometheus2.Prometheus2","title":"Prometheus2","text":"<p>A class that implements the Prometheus2 evaluation model for assessing LLM responses.</p> <p>This class provides methods for both relative and absolute evaluation of LLM responses using different rubrics such as factual validity, helpfulness, honesty, and reasoning.</p> Source code in <code>openpo/resources/prometheus2/prometheus2.py</code> <pre><code>class Prometheus2:\n    \"\"\"\n    A class that implements the Prometheus2 evaluation model for assessing LLM responses.\n\n    This class provides methods for both relative and absolute evaluation of LLM responses\n    using different rubrics such as factual validity, helpfulness, honesty, and reasoning.\n\n    \"\"\"\n\n    def __init__(self, model):\n        try:\n            from prometheus_eval import PrometheusEval\n            from prometheus_eval.prompts import (\n                ABSOLUTE_PROMPT_WO_REF,\n                FACTUAL_VALIDITY_RUBRIC,\n                HARMLESSNESS_RUBRIC,\n                HELPFULNESS_RUBRIC,\n                HONESTY_RUBRIC,\n                REASONING_RUBRIC,\n                RELATIVE_PROMPT_WO_REF,\n            )\n\n            self.PrometheusEval = PrometheusEval\n            self.ABSOLUTE_PROMPT_WO_REF = ABSOLUTE_PROMPT_WO_REF\n            self.RELATIVE_PROMPT_WO_REF = RELATIVE_PROMPT_WO_REF\n        except ImportError:\n            raise ImportError(\n                \"Prometheus2 requires additional dependencies. Install with: pip install openpo[eval]\"\n            )\n        try:\n            self.model = VLLM(model=model)\n        except Exception as e:\n            raise ProviderError(\n                \"Prometheus2\",\n                message=f\"failed to initialize Prometheus model: {str(e)}\",\n            )\n        self.rubric_mapping = {\n            \"factual-validity\": FACTUAL_VALIDITY_RUBRIC,\n            \"helpfulness\": HELPFULNESS_RUBRIC,\n            \"honesty\": HONESTY_RUBRIC,\n            \"harmlessness\": HARMLESSNESS_RUBRIC,\n            \"reasoning\": REASONING_RUBRIC,\n        }\n\n    def _format_absolute(\n        self,\n        scores: List[int],\n        instructions: List[str],\n        responses: List[str],\n        feedbacks: List[str],\n    ) -&gt; List[dict]:\n\n        dataset = []\n        for i in range(len(instructions)):\n            dataset.append(\n                {\n                    \"prompt\": instructions[i],\n                    \"response\": responses[i],\n                    \"score\": scores[i],\n                    \"feedback\": feedbacks[i],\n                }\n            )\n\n        return dataset\n\n    def _format_relative(\n        self,\n        instructions: List[str],\n        responses_A: List[str],\n        responses_B: List[str],\n        feedbacks: List[str],\n        scores: List[str],\n    ) -&gt; List[dict]:\n\n        dataset = []\n\n        for i in range(len(instructions)):\n            dataset.append(\n                {\n                    \"prompt\": instructions[i],\n                    \"score\": scores[i],\n                    \"preferred\": responses_A[i] if scores[i] == \"A\" else responses_B[i],\n                    \"rejected\": responses_A[i] if scores[i] == \"B\" else responses_B[i],\n                    \"feedback\": feedbacks[i],\n                }\n            )\n\n        return dataset\n\n    def eval_relative(\n        self,\n        instructions: List[str],\n        responses_A: List[str],\n        responses_B: List[str],\n        rubric: str,\n    ) -&gt; List[dict]:\n        \"\"\"\n        Performs relative evaluation comparing pairs of responses.\n\n        Args:\n            instructions (List[str]): List of instruction prompts.\n            responses_A (List[str]): First set of responses to compare.\n            responses_B (List[str]): Second set of responses to compare.\n            rubric (str): The evaluation rubric to use. Supported rubrics:\n\n                - factual-validity\n                - helpfulness\n                - honesty\n                - harmlessness\n                - reasoning\n\n        Returns:\n            List[dict]: A formatted list of evaluation results containing preferences and feedback.\n\n        Raises:\n            Exception: If there's an error during the evaluation process.\n        \"\"\"\n        try:\n            judge = self.PrometheusEval(\n                model=self.model,\n                relative_grade_template=self.RELATIVE_PROMPT_WO_REF,\n            )\n            feedbacks, scores = judge.relative_grade(\n                instructions=instructions,\n                responses_A=responses_A,\n                responses_B=responses_B,\n                rubric=self.rubric_mapping[rubric],\n            )\n\n            return self._format_relative(\n                instructions=instructions,\n                responses_A=responses_A,\n                responses_B=responses_B,\n                feedbacks=feedbacks,\n                scores=scores,\n            )\n\n        except Exception as e:\n            raise Exception(f\"Error while evaluating with Prometheus2: {e}\")\n\n    def eval_absolute(\n        self,\n        instructions: List[str],\n        responses: List[str],\n        rubric: str,\n    ) -&gt; List[dict]:\n        \"\"\"\n        Performs absolute evaluation of individual responses.\n\n        Args:\n            instructions (List[str]): List of instruction prompts.\n            responses (List[str]): List of responses to evaluate.\n            rubric (str): The evaluation rubric to use. Supported rubrics:\n\n                - factual-validity\n                - helpfulness\n                - honesty\n                - harmlessness\n                - reasoning\n\n        Returns:\n            List[dict]: A formatted list of evaluation results containing scores and feedback.\n\n        Raises:\n            Exception: If there's an error during the evaluation process.\n        \"\"\"\n        try:\n            judge = self.PrometheusEval(\n                model=self.model,\n                absolute_grade_template=self.ABSOLUTE_PROMPT_WO_REF,\n            )\n            feedbacks, scores = judge.absolute_grade(\n                instructions=instructions,\n                responses=responses,\n                rubric=self.rubric_mapping[rubric],\n            )\n\n            return self._format_absolute(\n                scores=scores,\n                instructions=instructions,\n                responses=responses,\n                feedbacks=feedbacks,\n            )\n        except Exception as e:\n            raise Exception(f\"Error while evaluating with Prometheus2: {e}\")\n</code></pre>"},{"location":"api/#openpo.resources.prometheus2.prometheus2.Prometheus2.eval_relative","title":"eval_relative","text":"<pre><code>eval_relative(instructions: List[str], responses_A: List[str], responses_B: List[str], rubric: str) -&gt; List[dict]\n</code></pre> <p>Performs relative evaluation comparing pairs of responses.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>List[str]</code> <p>List of instruction prompts.</p> required <code>responses_A</code> <code>List[str]</code> <p>First set of responses to compare.</p> required <code>responses_B</code> <code>List[str]</code> <p>Second set of responses to compare.</p> required <code>rubric</code> <code>str</code> <p>The evaluation rubric to use. Supported rubrics:</p> <ul> <li>factual-validity</li> <li>helpfulness</li> <li>honesty</li> <li>harmlessness</li> <li>reasoning</li> </ul> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: A formatted list of evaluation results containing preferences and feedback.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error during the evaluation process.</p> Source code in <code>openpo/resources/prometheus2/prometheus2.py</code> <pre><code>def eval_relative(\n    self,\n    instructions: List[str],\n    responses_A: List[str],\n    responses_B: List[str],\n    rubric: str,\n) -&gt; List[dict]:\n    \"\"\"\n    Performs relative evaluation comparing pairs of responses.\n\n    Args:\n        instructions (List[str]): List of instruction prompts.\n        responses_A (List[str]): First set of responses to compare.\n        responses_B (List[str]): Second set of responses to compare.\n        rubric (str): The evaluation rubric to use. Supported rubrics:\n\n            - factual-validity\n            - helpfulness\n            - honesty\n            - harmlessness\n            - reasoning\n\n    Returns:\n        List[dict]: A formatted list of evaluation results containing preferences and feedback.\n\n    Raises:\n        Exception: If there's an error during the evaluation process.\n    \"\"\"\n    try:\n        judge = self.PrometheusEval(\n            model=self.model,\n            relative_grade_template=self.RELATIVE_PROMPT_WO_REF,\n        )\n        feedbacks, scores = judge.relative_grade(\n            instructions=instructions,\n            responses_A=responses_A,\n            responses_B=responses_B,\n            rubric=self.rubric_mapping[rubric],\n        )\n\n        return self._format_relative(\n            instructions=instructions,\n            responses_A=responses_A,\n            responses_B=responses_B,\n            feedbacks=feedbacks,\n            scores=scores,\n        )\n\n    except Exception as e:\n        raise Exception(f\"Error while evaluating with Prometheus2: {e}\")\n</code></pre>"},{"location":"api/#openpo.resources.prometheus2.prometheus2.Prometheus2.eval_absolute","title":"eval_absolute","text":"<pre><code>eval_absolute(instructions: List[str], responses: List[str], rubric: str) -&gt; List[dict]\n</code></pre> <p>Performs absolute evaluation of individual responses.</p> <p>Parameters:</p> Name Type Description Default <code>instructions</code> <code>List[str]</code> <p>List of instruction prompts.</p> required <code>responses</code> <code>List[str]</code> <p>List of responses to evaluate.</p> required <code>rubric</code> <code>str</code> <p>The evaluation rubric to use. Supported rubrics:</p> <ul> <li>factual-validity</li> <li>helpfulness</li> <li>honesty</li> <li>harmlessness</li> <li>reasoning</li> </ul> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: A formatted list of evaluation results containing scores and feedback.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error during the evaluation process.</p> Source code in <code>openpo/resources/prometheus2/prometheus2.py</code> <pre><code>def eval_absolute(\n    self,\n    instructions: List[str],\n    responses: List[str],\n    rubric: str,\n) -&gt; List[dict]:\n    \"\"\"\n    Performs absolute evaluation of individual responses.\n\n    Args:\n        instructions (List[str]): List of instruction prompts.\n        responses (List[str]): List of responses to evaluate.\n        rubric (str): The evaluation rubric to use. Supported rubrics:\n\n            - factual-validity\n            - helpfulness\n            - honesty\n            - harmlessness\n            - reasoning\n\n    Returns:\n        List[dict]: A formatted list of evaluation results containing scores and feedback.\n\n    Raises:\n        Exception: If there's an error during the evaluation process.\n    \"\"\"\n    try:\n        judge = self.PrometheusEval(\n            model=self.model,\n            absolute_grade_template=self.ABSOLUTE_PROMPT_WO_REF,\n        )\n        feedbacks, scores = judge.absolute_grade(\n            instructions=instructions,\n            responses=responses,\n            rubric=self.rubric_mapping[rubric],\n        )\n\n        return self._format_absolute(\n            scores=scores,\n            instructions=instructions,\n            responses=responses,\n            feedbacks=feedbacks,\n        )\n    except Exception as e:\n        raise Exception(f\"Error while evaluating with Prometheus2: {e}\")\n</code></pre>"},{"location":"api/#storage","title":"Storage","text":""},{"location":"api/#openpo.storage.huggingface.HuggingFaceStorage","title":"HuggingFaceStorage","text":"<p>Storage class for HuggingFace Datasets.</p> <p>This class provides methods to store and retrieve data from HuggingFace's dataset repositories. It handles the creation of repositories and manages data upload and download operations.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>HuggingFace API token with write access. Environment variable can be set instead of passing in the key.</p> <code>None</code> <p>Raises:</p> Type Description <code>AuthenticationError</code> <p>If authentication fails</p> <code>ProviderError</code> <p>If HuggingFace error is raised</p> Source code in <code>openpo/storage/huggingface.py</code> <pre><code>class HuggingFaceStorage:\n    \"\"\"Storage class for HuggingFace Datasets.\n\n    This class provides methods to store and retrieve data from HuggingFace's dataset\n    repositories. It handles the creation of repositories and manages data upload\n    and download operations.\n\n    Parameters:\n        api_key (str): HuggingFace API token with write access. Environment variable can be set instead of passing in the key.\n\n    Raises:\n        AuthenticationError: If authentication fails\n        ProviderError: If HuggingFace error is raised\n    \"\"\"\n\n    def __init__(self, api_key: Optional[str] = None):\n        self.api_key = api_key or os.getenv(\"HF_API_KEY\")\n        if not self.api_key:\n            raise AuthenticationError(\n                provider=\"Huggingface\",\n                message=f\"HuggingFace API key must be provided.\",\n            )\n\n    def _convert_to_dict(self, data: List[Dict[str, Any]]) -&gt; Dict:\n        if not data:\n            return {}\n\n        keys = data[0].keys()\n\n        return {key: [item[key] for item in data] for key in keys}\n\n    def push_to_repo(\n        self,\n        repo_id: str,\n        data: Union[List[Dict[str, Any]], pd.DataFrame],\n        config_name: str = \"default\",\n        set_default: Optional[bool] = None,\n        split: Optional[str] = None,\n        data_dir: Optional[str] = None,\n        commit_message: Optional[str] = None,\n        commit_description: Optional[str] = None,\n        private: Optional[bool] = False,\n        token: Optional[str] = None,\n        revision: Optional[str] = None,\n        create_pr: Optional[bool] = False,\n        max_shard_size: Optional[Union[int, str]] = None,\n        num_shards: Optional[int] = None,\n        embed_external_files: bool = True,\n    ):\n        \"\"\"\n        Push data to HuggingFace dataset repository.\n        This is the implementation of HuggingFace Dataset's push_to_hub method.\n        For parameters not listed, check HuggingFace documentation for more detail.\n\n        Args:\n            data: The data to upload.\n\n                - List[Dict]\n                - pandas DataFrame\n\n            repo_id (str): Name of the dataset repository.\n\n        Raises:\n            Exception: If pushing to dataset repository fails.\n        \"\"\"\n\n        if not isinstance(data, (list, pd.DataFrame)):\n            raise TypeError(\"data must be a list of dictionaries or pandas DataFrame\")\n\n        if isinstance(data, pd.DataFrame):\n            ds = Dataset.from_pandas(data)\n\n        if isinstance(data, list):\n            ds = self._convert_to_dict(data)\n            ds = Dataset.from_dict(ds)\n\n        try:\n            ds.push_to_hub(\n                repo_id=repo_id,\n                config_name=config_name,\n                set_default=set_default,\n                split=split,\n                data_dir=data_dir,\n                commit_message=commit_message,\n                commit_description=commit_description,\n                private=private,\n                token=token,\n                revision=revision,\n                create_pr=create_pr,\n                max_shard_size=max_shard_size,\n                num_shards=num_shards,\n                embed_external_files=embed_external_files,\n            )\n        except Exception as e:\n            raise ProviderError(\n                provider=\"huggingface storage\",\n                message=f\"Error pushing data to the repository: {str(e)}\",\n            )\n\n    def load_from_repo(\n        self,\n        path: str,\n        name: Optional[str] = None,\n        data_dir: Optional[str] = None,\n        data_files: Optional[\n            Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]]\n        ] = None,\n        split: Optional[str] = None,\n        cache_dir: Optional[str] = None,\n        features=None,\n        download_config=None,\n        download_mode=None,\n        verification_mode=None,\n        keep_in_memory: Optional[bool] = None,\n        save_infos: bool = False,\n        revision: Optional[str] = None,\n        token: Optional[Union[bool, str]] = None,\n        streaming: bool = False,\n        num_proc: Optional[int] = None,\n        storage_options: Optional[Dict] = None,\n        trust_remote_code: bool = None,\n        **config_kwargs,\n    ) -&gt; Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset]:\n        \"\"\"\n        Load data from HuggingFace dataset repository.\n        This is direct implementation of HuggingFace Dataset load_dataset method.\n        For arguments not listed here, check HuggingFace documentation for more detail.\n\n        Args:\n            path (str): Path or name of the dataset.\n\n        Raises:\n            Exception: If loading data from repository fails.\n        \"\"\"\n\n        try:\n            return load_dataset(\n                path=path,\n                name=name,\n                data_dir=data_dir,\n                data_files=data_files,\n                split=split,\n                cache_dir=cache_dir,\n                features=features,\n                download_config=download_config,\n                download_mode=download_mode,\n                verification_mode=verification_mode,\n                keep_in_memory=keep_in_memory,\n                save_infos=save_infos,\n                revision=revision,\n                token=token,\n                streaming=streaming,\n                num_proc=num_proc,\n                storage_options=storage_options,\n                trust_remote_code=trust_remote_code,\n                **config_kwargs,\n            )\n        except Exception as e:\n            raise ProviderError(\n                provider=\"huggingface storage\",\n                message=f\"Error loading data from the HF repository: {str(e)}\",\n            )\n</code></pre>"},{"location":"api/#openpo.storage.huggingface.HuggingFaceStorage.push_to_repo","title":"push_to_repo","text":"<pre><code>push_to_repo(repo_id: str, data: Union[List[Dict[str, Any]], pd.DataFrame], config_name: str = 'default', set_default: Optional[bool] = None, split: Optional[str] = None, data_dir: Optional[str] = None, commit_message: Optional[str] = None, commit_description: Optional[str] = None, private: Optional[bool] = False, token: Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] = False, max_shard_size: Optional[Union[int, str]] = None, num_shards: Optional[int] = None, embed_external_files: bool = True)\n</code></pre> <p>Push data to HuggingFace dataset repository. This is the implementation of HuggingFace Dataset's push_to_hub method. For parameters not listed, check HuggingFace documentation for more detail.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[List[Dict[str, Any]], DataFrame]</code> <p>The data to upload.</p> <ul> <li>List[Dict]</li> <li>pandas DataFrame</li> </ul> required <code>repo_id</code> <code>str</code> <p>Name of the dataset repository.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If pushing to dataset repository fails.</p> Source code in <code>openpo/storage/huggingface.py</code> <pre><code>def push_to_repo(\n    self,\n    repo_id: str,\n    data: Union[List[Dict[str, Any]], pd.DataFrame],\n    config_name: str = \"default\",\n    set_default: Optional[bool] = None,\n    split: Optional[str] = None,\n    data_dir: Optional[str] = None,\n    commit_message: Optional[str] = None,\n    commit_description: Optional[str] = None,\n    private: Optional[bool] = False,\n    token: Optional[str] = None,\n    revision: Optional[str] = None,\n    create_pr: Optional[bool] = False,\n    max_shard_size: Optional[Union[int, str]] = None,\n    num_shards: Optional[int] = None,\n    embed_external_files: bool = True,\n):\n    \"\"\"\n    Push data to HuggingFace dataset repository.\n    This is the implementation of HuggingFace Dataset's push_to_hub method.\n    For parameters not listed, check HuggingFace documentation for more detail.\n\n    Args:\n        data: The data to upload.\n\n            - List[Dict]\n            - pandas DataFrame\n\n        repo_id (str): Name of the dataset repository.\n\n    Raises:\n        Exception: If pushing to dataset repository fails.\n    \"\"\"\n\n    if not isinstance(data, (list, pd.DataFrame)):\n        raise TypeError(\"data must be a list of dictionaries or pandas DataFrame\")\n\n    if isinstance(data, pd.DataFrame):\n        ds = Dataset.from_pandas(data)\n\n    if isinstance(data, list):\n        ds = self._convert_to_dict(data)\n        ds = Dataset.from_dict(ds)\n\n    try:\n        ds.push_to_hub(\n            repo_id=repo_id,\n            config_name=config_name,\n            set_default=set_default,\n            split=split,\n            data_dir=data_dir,\n            commit_message=commit_message,\n            commit_description=commit_description,\n            private=private,\n            token=token,\n            revision=revision,\n            create_pr=create_pr,\n            max_shard_size=max_shard_size,\n            num_shards=num_shards,\n            embed_external_files=embed_external_files,\n        )\n    except Exception as e:\n        raise ProviderError(\n            provider=\"huggingface storage\",\n            message=f\"Error pushing data to the repository: {str(e)}\",\n        )\n</code></pre>"},{"location":"api/#openpo.storage.huggingface.HuggingFaceStorage.load_from_repo","title":"load_from_repo","text":"<pre><code>load_from_repo(path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Optional[Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]]] = None, split: Optional[str] = None, cache_dir: Optional[str] = None, features=None, download_config=None, download_mode=None, verification_mode=None, keep_in_memory: Optional[bool] = None, save_infos: bool = False, revision: Optional[str] = None, token: Optional[Union[bool, str]] = None, streaming: bool = False, num_proc: Optional[int] = None, storage_options: Optional[Dict] = None, trust_remote_code: bool = None, **config_kwargs) -&gt; Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset]\n</code></pre> <p>Load data from HuggingFace dataset repository. This is direct implementation of HuggingFace Dataset load_dataset method. For arguments not listed here, check HuggingFace documentation for more detail.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path or name of the dataset.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If loading data from repository fails.</p> Source code in <code>openpo/storage/huggingface.py</code> <pre><code>def load_from_repo(\n    self,\n    path: str,\n    name: Optional[str] = None,\n    data_dir: Optional[str] = None,\n    data_files: Optional[\n        Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]]\n    ] = None,\n    split: Optional[str] = None,\n    cache_dir: Optional[str] = None,\n    features=None,\n    download_config=None,\n    download_mode=None,\n    verification_mode=None,\n    keep_in_memory: Optional[bool] = None,\n    save_infos: bool = False,\n    revision: Optional[str] = None,\n    token: Optional[Union[bool, str]] = None,\n    streaming: bool = False,\n    num_proc: Optional[int] = None,\n    storage_options: Optional[Dict] = None,\n    trust_remote_code: bool = None,\n    **config_kwargs,\n) -&gt; Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset]:\n    \"\"\"\n    Load data from HuggingFace dataset repository.\n    This is direct implementation of HuggingFace Dataset load_dataset method.\n    For arguments not listed here, check HuggingFace documentation for more detail.\n\n    Args:\n        path (str): Path or name of the dataset.\n\n    Raises:\n        Exception: If loading data from repository fails.\n    \"\"\"\n\n    try:\n        return load_dataset(\n            path=path,\n            name=name,\n            data_dir=data_dir,\n            data_files=data_files,\n            split=split,\n            cache_dir=cache_dir,\n            features=features,\n            download_config=download_config,\n            download_mode=download_mode,\n            verification_mode=verification_mode,\n            keep_in_memory=keep_in_memory,\n            save_infos=save_infos,\n            revision=revision,\n            token=token,\n            streaming=streaming,\n            num_proc=num_proc,\n            storage_options=storage_options,\n            trust_remote_code=trust_remote_code,\n            **config_kwargs,\n        )\n    except Exception as e:\n        raise ProviderError(\n            provider=\"huggingface storage\",\n            message=f\"Error loading data from the HF repository: {str(e)}\",\n        )\n</code></pre>"},{"location":"api/#openpo.storage.s3.S3Storage","title":"S3Storage","text":"<p>Storage adapter for Amazon S3.</p> <p>This class provides methods to store and retrieve data from Amazon S3 buckets. It handles JSON serialization/deserialization and manages S3 operations through boto3 client.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments can be passed to access AWS:</p> <ul> <li>region_name</li> <li>aws_access_key_id</li> <li>aws_secret_access_key</li> <li>profile_name</li> </ul> <p>Alternatively, credentials can be configured with aws configure</p> <code>{}</code> <p>Raises:</p> Type Description <code>ProviderError</code> <p>If S3 Client error is raised</p> Source code in <code>openpo/storage/s3.py</code> <pre><code>class S3Storage:\n    \"\"\"Storage adapter for Amazon S3.\n\n    This class provides methods to store and retrieve data from Amazon S3 buckets.\n    It handles JSON serialization/deserialization and manages S3 operations through\n    boto3 client.\n\n    Parameters:\n        **kwargs: Keyword arguments can be passed to access AWS:\n\n            - region_name\n            - aws_access_key_id\n            - aws_secret_access_key\n            - profile_name\n\n            Alternatively, credentials can be configured with aws configure\n\n    Raises:\n        ProviderError: If S3 Client error is raised\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        try:\n            self.s3 = boto3.client(\"s3\", **kwargs)\n        except ClientError as e:\n            raise ProviderError(\n                provider=\"s3\",\n                message=f\"Failed to initialize boto3 client: {str(e)}\",\n            )\n\n    def _read_file(self, bucket: str, key: str) -&gt; List[Dict[str, Any]]:\n        try:\n            res = self.s3.get_object(Bucket=bucket, Key=key)\n            content = res[\"Body\"].read()\n\n            file_ext = key.split(\".\")[-1].lower()\n            if file_ext == \"json\":\n                data = json.loads(content)\n                if isinstance(data, list):\n                    return data\n                return list(data)\n            elif file_ext == \"parquet\":\n                try:\n                    parquet_buffer = io.BytesIO(content)\n                    df = pd.read_parquet(parquet_buffer)\n                    return json.loads(df.to_json(orient=\"records\"))\n                except Exception as e:\n                    raise ValueError(f\"Failed to parse content as parquet: {str(e)}\")\n            else:\n                raise ValueError(\n                    f\"Unsupported content type: {content_type}. Supported extensions are: json, parquet \"\n                )\n        except ClientError as err:\n            raise err\n\n    def _serialize_data(\n        self,\n        data,\n        serialization_type,\n    ) -&gt; tuple[bytes, str]:\n        if isinstance(data, bytes):\n            return data, \"application/octet-stream\"\n\n        if serialization_type == \"parquet\":\n            buffer = io.BytesIO()\n\n            if isinstance(data, list):\n                if not all(isinstance(item, dict) for item in data):\n                    raise TypeError(\n                        \"All items in list must be dictionaries when using 'parquet' serialization\"\n                    )\n                df = pd.DataFrame(data)\n            elif isinstance(data, pd.DataFrame):\n                df = data\n            else:\n                raise TypeError(\n                    \"Data must be DataFrame or list of dicts when using 'parquet' serialization\"\n                )\n\n            df.to_parquet(buffer)\n            return buffer.getvalue(), \"application/octet-stream\"\n\n        if serialization_type == \"json\":\n            if isinstance(data, pd.DataFrame):\n                data = json.loads(data.to_json(orient=\"records\"))\n            elif not isinstance(data, list):\n                raise TypeError(\n                    \"Data must be a list or DataFrame when using 'json' serialization\"\n                )\n\n            return json.dumps(data, default=str).encode(), \"application/json\"\n\n        raise ValueError(f\"Unsupported serialization type: {serialization_type}\")\n\n    def push_to_s3(\n        self,\n        data: Union[List[Dict[str, Any]], pd.DataFrame, bytes],\n        bucket: str,\n        key: Optional[str] = None,\n        ext_type: Literal[\"parquet\", \"json\"] = \"parquet\",\n    ):\n        \"\"\"Upload data to an S3 bucket.\n\n        Args:\n            data: The data to upload.\n\n                - List[Dict]: List of dictionaries\n                - pd.DataFrame: Pandas DataFrame\n\n            bucket (str): Name of the S3 bucket\n            key (str, optional): Object key (path) in the bucket\n\n            ext_type (str): Type of serialization to use:\n\n                - parquet: Serialize as parquet\n                - json: Serialize as JSON\n\n\n        Raises:\n            ClientError: If S3 operation fails\n            TypeError: If data type is not compatible with chosen serialization type\n            ValueError: If serialization type is not supported or data cannot be deserialized\n        \"\"\"\n        try:\n            serialized_data, content_type = self._serialize_data(data, ext_type)\n\n            self.s3.put_object(\n                Bucket=bucket,\n                Key=f\"{key}.{ext_type}\",\n                Body=serialized_data,\n                ContentType=content_type,\n            )\n\n        except ClientError as err:\n            raise ProviderError(\n                provider=\"s3\",\n                message=f\"Failed to push data to s3: {str(err)}\",\n            )\n\n    def load_from_s3(self, bucket: str, key: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"Read data from an S3 bucket.\n\n        Args:\n            bucket (str): Name of the S3 bucket.\n            key (str): Object name (path) in the bucket.\n\n        Returns:\n            List[Dict]: The loaded data as a list of dictionaries.\n\n        Raises:\n            ClientError: If S3 operation fails.\n            ValueError: If content type is not supported or content cannot be parsed.\n        \"\"\"\n        content = self._read_file(bucket, key)\n        return content\n</code></pre>"},{"location":"api/#openpo.storage.s3.S3Storage.push_to_s3","title":"push_to_s3","text":"<pre><code>push_to_s3(data: Union[List[Dict[str, Any]], pd.DataFrame, bytes], bucket: str, key: Optional[str] = None, ext_type: Literal['parquet', 'json'] = 'parquet')\n</code></pre> <p>Upload data to an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[List[Dict[str, Any]], DataFrame, bytes]</code> <p>The data to upload.</p> <ul> <li>List[Dict]: List of dictionaries</li> <li>pd.DataFrame: Pandas DataFrame</li> </ul> required <code>bucket</code> <code>str</code> <p>Name of the S3 bucket</p> required <code>key</code> <code>str</code> <p>Object key (path) in the bucket</p> <code>None</code> <code>ext_type</code> <code>str</code> <p>Type of serialization to use:</p> <ul> <li>parquet: Serialize as parquet</li> <li>json: Serialize as JSON</li> </ul> <code>'parquet'</code> <p>Raises:</p> Type Description <code>ClientError</code> <p>If S3 operation fails</p> <code>TypeError</code> <p>If data type is not compatible with chosen serialization type</p> <code>ValueError</code> <p>If serialization type is not supported or data cannot be deserialized</p> Source code in <code>openpo/storage/s3.py</code> <pre><code>def push_to_s3(\n    self,\n    data: Union[List[Dict[str, Any]], pd.DataFrame, bytes],\n    bucket: str,\n    key: Optional[str] = None,\n    ext_type: Literal[\"parquet\", \"json\"] = \"parquet\",\n):\n    \"\"\"Upload data to an S3 bucket.\n\n    Args:\n        data: The data to upload.\n\n            - List[Dict]: List of dictionaries\n            - pd.DataFrame: Pandas DataFrame\n\n        bucket (str): Name of the S3 bucket\n        key (str, optional): Object key (path) in the bucket\n\n        ext_type (str): Type of serialization to use:\n\n            - parquet: Serialize as parquet\n            - json: Serialize as JSON\n\n\n    Raises:\n        ClientError: If S3 operation fails\n        TypeError: If data type is not compatible with chosen serialization type\n        ValueError: If serialization type is not supported or data cannot be deserialized\n    \"\"\"\n    try:\n        serialized_data, content_type = self._serialize_data(data, ext_type)\n\n        self.s3.put_object(\n            Bucket=bucket,\n            Key=f\"{key}.{ext_type}\",\n            Body=serialized_data,\n            ContentType=content_type,\n        )\n\n    except ClientError as err:\n        raise ProviderError(\n            provider=\"s3\",\n            message=f\"Failed to push data to s3: {str(err)}\",\n        )\n</code></pre>"},{"location":"api/#openpo.storage.s3.S3Storage.load_from_s3","title":"load_from_s3","text":"<pre><code>load_from_s3(bucket: str, key: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Read data from an S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the S3 bucket.</p> required <code>key</code> <code>str</code> <p>Object name (path) in the bucket.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict]: The loaded data as a list of dictionaries.</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>If S3 operation fails.</p> <code>ValueError</code> <p>If content type is not supported or content cannot be parsed.</p> Source code in <code>openpo/storage/s3.py</code> <pre><code>def load_from_s3(self, bucket: str, key: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"Read data from an S3 bucket.\n\n    Args:\n        bucket (str): Name of the S3 bucket.\n        key (str): Object name (path) in the bucket.\n\n    Returns:\n        List[Dict]: The loaded data as a list of dictionaries.\n\n    Raises:\n        ClientError: If S3 operation fails.\n        ValueError: If content type is not supported or content cannot be parsed.\n    \"\"\"\n    content = self._read_file(bucket, key)\n    return content\n</code></pre>"},{"location":"completions/","title":"Completion","text":"<p>Completion is the building block of synthetic data generation. It allows you to easily generate outputs from any LLM of your choice.</p>"},{"location":"completions/#using-api","title":"Using API","text":"<p>Easiest way to generate outputs from LLMs is via API. OpenPO provides OpenAI compatible interface to make request to various endpoints to gather outputs.</p> <p>OpenPO supports various model parameters. You can pass them in as a dictionary to <code>params</code></p> <pre><code>response = client.completion.generate(\n    model=\"huggingface/Qwen/Qwen2.5-Coder-32B-Instruct\",\n    messages=messages,\n    params = {\n        \"temperature\": 1.0,\n        \"seed\": 42,\n        \"max_tokens\": 1000,\n    }\n)\n</code></pre> <p>List of all available model parameter is available in the parameters section</p>"},{"location":"completions/#using-vllm","title":"Using vLLM","text":"<p>For high performance local inference, OpenPO supports vLLM engine. To get started with vLLM, load the model using built-in vLLM class.</p> <p>Note</p> <p>vLLM requires appropriate hardware and GPU to load models and make inference locally.</p> <pre><code>from openpo import VLLM\n\nllm = VLLM(model=\"Qwen/Qwen2-0.5B-Instruct\")\nres = llm.generate(messages=messages)\n</code></pre> <p>You can configure VLLM instance as well as provide model parameters.</p> <pre><code>llm = VLLM(\n    model=\"Qwen/Qwen2-0.5B-Instruct\",\n    tokenizer=tokenizer,\n    dtype='bfloat16',\n    gpu_memory_utilization=0.95,\n)\n\nres = llm.generate(\n    messages=messages,\n    chat_template=chat_template,\n    sampling_params={\n        \"temperature\": 0.8,\n        \"max_tokens\": 1000,\n    }\n)\n</code></pre> <p>For more information on parameters, refer to the API reference</p>"},{"location":"errors/","title":"Errors","text":"<p>List of error classes OpenPO implements.</p>"},{"location":"errors/#openpo.internal.error.APIError","title":"APIError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for API-related errors.</p> <p>This class serves as the base for all API-related exceptions in the openpo library. It provides detailed error information including status codes, response data, and specific error messages.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Human-readable error message describing the issue</p> required <code>status_code</code> <code>Optional[int]</code> <p>HTTP status code associated with the error</p> <code>None</code> <code>response</code> <code>Optional[Dict]</code> <p>Raw response data from the API</p> <code>None</code> <code>error</code> <code>Optional[str]</code> <p>Specific error code or identifier</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>The error message</p> <code>status_code</code> <code>Optional[int]</code> <p>The HTTP status code</p> <code>response</code> <code>Optional[Dict]</code> <p>The raw API response</p> <code>error</code> <code>Optional[str]</code> <p>The error identifier</p> Source code in <code>openpo/internal/error.py</code> <pre><code>class APIError(Exception):\n    \"\"\"Base exception class for API-related errors.\n\n    This class serves as the base for all API-related exceptions in the openpo library.\n    It provides detailed error information including status codes, response data, and\n    specific error messages.\n\n    Args:\n        message (str): Human-readable error message describing the issue\n        status_code (Optional[int]): HTTP status code associated with the error\n        response (Optional[Dict]): Raw response data from the API\n        error (Optional[str]): Specific error code or identifier\n\n    Attributes:\n        message (str): The error message\n        status_code (Optional[int]): The HTTP status code\n        response (Optional[Dict]): The raw API response\n        error (Optional[str]): The error identifier\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        status_code: Optional[int] = None,\n        response: Optional[Dict] = None,\n        error: Optional[str] = None,\n    ):\n        self.message = message\n        self.status_code = status_code\n        self.response = response\n        self.error = error\n\n        super().__init__(message)\n</code></pre>"},{"location":"errors/#openpo.internal.error.AuthenticationError","title":"AuthenticationError","text":"<p>               Bases: <code>APIError</code></p> <p>Exception raised for API authentication failures.</p> <p>This exception is raised when there are issues with API authentication, such as invalid API keys, expired tokens, or missing credentials.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the provider (e.g., 'OpenAI', 'Anthropic')</p> required <code>message</code> <code>Optional[str]</code> <p>Custom error message. If not provided, a default message will be generated using the provider name</p> <code>None</code> <code>status_code</code> <code>Optional[int]</code> <p>HTTP status code from the authentication attempt</p> <code>None</code> <code>response</code> <code>Optional[Dict]</code> <p>Raw response data from the authentication attempt</p> <code>None</code> Source code in <code>openpo/internal/error.py</code> <pre><code>class AuthenticationError(APIError):\n    \"\"\"Exception raised for API authentication failures.\n\n    This exception is raised when there are issues with API authentication, such as\n    invalid API keys, expired tokens, or missing credentials.\n\n    Args:\n        provider (str): Name of the provider (e.g., 'OpenAI', 'Anthropic')\n        message (Optional[str]): Custom error message. If not provided, a default\n            message will be generated using the provider name\n        status_code (Optional[int]): HTTP status code from the authentication attempt\n        response (Optional[Dict]): Raw response data from the authentication attempt\n    \"\"\"\n\n    def __init__(\n        self,\n        provider: str,\n        message: Optional[str] = None,\n        status_code: Optional[int] = None,\n        response: Optional[Dict] = None,\n    ):\n        error_msg = (\n            message if message else f\"{provider} API key is invalid or not provided\"\n        )\n        super().__init__(\n            message=error_msg,\n            status_code=status_code,\n            response=response,\n            error=\"authentication_error\",\n        )\n</code></pre>"},{"location":"errors/#openpo.internal.error.ProviderError","title":"ProviderError","text":"<p>               Bases: <code>APIError</code></p> <p>Exception raised for provider-specific errors.</p> <p>This exception is raised when a provider returns an error that is specific to their service, such as rate limits, invalid model names, or service-specific validation errors.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the provider (e.g., 'OpenAI', 'Anthropic')</p> required <code>message</code> <code>str</code> <p>Detailed error message from the provider</p> required <code>status_code</code> <code>Optional[int]</code> <p>HTTP status code from the provider</p> <code>None</code> <code>response</code> <code>Optional[Dict]</code> <p>Raw response data from the provider</p> <code>None</code> Source code in <code>openpo/internal/error.py</code> <pre><code>class ProviderError(APIError):\n    \"\"\"Exception raised for provider-specific errors.\n\n    This exception is raised when a provider returns an error that is specific to\n    their service, such as rate limits, invalid model names, or service-specific\n    validation errors.\n\n    Args:\n        provider (str): Name of the provider (e.g., 'OpenAI', 'Anthropic')\n        message (str): Detailed error message from the provider\n        status_code (Optional[int]): HTTP status code from the provider\n        response (Optional[Dict]): Raw response data from the provider\n    \"\"\"\n\n    def __init__(\n        self,\n        provider: str,\n        message: str,\n        status_code: Optional[int] = None,\n        response: Optional[Dict] = None,\n    ):\n        super().__init__(\n            message=f\"{provider} provider error: {message}\",\n            status_code=status_code,\n            response=response,\n            error=\"provider_error\",\n        )\n</code></pre>"},{"location":"errors/#openpo.internal.error.InvalidJSONFormatError","title":"InvalidJSONFormatError","text":"<p>               Bases: <code>JSONExtractionError</code></p> <p>Exception raised when JSON extraction or parsing fails.</p> <p>This exception is raised when attempting to extract or parse JSON content that is not properly formatted or is invalid according to JSON specifications.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[str]</code> <p>Custom error message. If not provided, a default message indicating invalid JSON format will be used.</p> <code>None</code> Source code in <code>openpo/internal/error.py</code> <pre><code>class InvalidJSONFormatError(JSONExtractionError):\n    \"\"\"Exception raised when JSON extraction or parsing fails.\n\n    This exception is raised when attempting to extract or parse JSON content\n    that is not properly formatted or is invalid according to JSON specifications.\n\n    Args:\n        message (Optional[str]): Custom error message. If not provided, a default\n            message indicating invalid JSON format will be used.\n    \"\"\"\n\n    def __init__(self, message: Optional[str] = None):\n        error_msg = message if message else \"The extracted text is not valid JSON\"\n        super().__init__(error_msg)\n</code></pre>"},{"location":"evaluations/","title":"Evaluation","text":"<p>Evaluation is the magic that synthesizes outputs data into finetuned ready dataset. To get started with evaluation, first install extra dependencies.</p> <pre><code>pip install openpo[eval]\n</code></pre>"},{"location":"evaluations/#llm-as-a-judge","title":"LLM-as-a-Judge","text":"<p>Since Constitutional AI: Harmlessness from AI Feedback from Anthropic established a groundwork for using AI feedback beyond just ensuring model safety, Subsequent researches have demonstrated the effectiveness of LLM-based evaluation for synthetic data generation, potentially offering a scalable alternative to human annotation.</p> <p>OpenPO adopts LLM-as-a-Judge methodology, supporting both single and multi-judge configurations to generate high-quality dataset.</p>"},{"location":"evaluations/#using-api","title":"Using API","text":"<p>Simplest way to run evaluation is to make request to the models using <code>eval</code> method.</p> <p>Note</p> <p>Evaluation currently suppports OpenAI and Anthropic models only.</p> <p><pre><code>from openpo import OpenPO\n\nopenpo = OpenPO()\n\nresponses = [\n    [\"Lorem ipsum dolor sit amet\", \"consectetur adipiscing elit\"],\n    [\" Aliquam pharetra neque\", \"ultricies elit imperdiet laoreet\"],\n]\n\nres = openpo.evaluate.eval(\n    model='openai/gpt-4o-mini\",\n    questions=questions,\n    responses=data,\n)\n</code></pre>  OpenPO supports multi-judge, where more than one model is used as a judge to reach consensus. Use list of model identifiers for this.</p> <p><pre><code>res = openpo.evaluate.eval(\n    model=['openai/gpt-4o', 'anthropic/claude-sonnet-3-5-latest'],\n    questions=questions,\n    responses=responses,\n)\n</code></pre>  If you want more control over the behavior of judge models, use custom prompt.</p> <pre><code>res = openpo.eval_single(\n    model=\"openai/gpt-4o\",\n    questions=questions,\n    responses=responses,\n    prompt=prompt,\n)\n</code></pre> <p>For more details, take a look at the API reference</p>"},{"location":"evaluations/#using-batch-processing","title":"Using Batch Processing","text":"<p>Using API to evaluate large volume of dataset can be costly and inefficient. Batch processing provides cost-effective way to handle tasks with large volume that do not require immediate responses.</p> <p>Running batch processing is similar to how evaluation with API works. To use single model evaluation, pass the model identifier to the models parameter.</p> <pre><code>openpo = OpenPO()\n\n# run batch processing\nbatch_info = openpo.batch.eval(\n    model='openai/gpt-4o-mini',\n    questions=questions,\n    responses=responses,\n)\n\n# check batch status\nstatus = openpo.batch.check_status(batch_info.id)\n\n# load batch result\nresult = openpo.batch.load_batch(\n    filename=status.output_file_id,\n    provider='openai',\n)\n</code></pre> <p>Using multi-judge with batch processing is very similar:</p> <pre><code>batch_a_info, batch_b_info = openpo.batch.eval(\n    model=[\"openai/gpt-4o-mini\", \"anthropic/anthropic/claude-3-5-haiku-20241022\"],\n    questions=questions,\n    responses=responses,\n)\n\n# load batch results once done\nbatch_a = openpo.batch.load_batch(filename=batch_a_info.output_file_id, provider='openai')\nbatch_b = openpo.batch.load_batch(filename=batch_b_info.id, provider'anthropic')\n\n# get consensus\nresult = openpo.batch.get_consensus(\n    batch_A=batch_a,\n    batch_B=batch_b,\n)\n</code></pre> <p>OpenAI and Anthropic provide UI for batch processing jobs. Visit their console to find more information such as status, metadata and more.</p>"},{"location":"evaluations/#evaluation-models","title":"Evaluation Models","text":"<p>Note</p> <p>Evaluation Models require to run on appropriate hardware with GPU and memory to make inference.</p> <p>Fine-tuned evaluation models offer an efficient approach to synthesizing response datasets. These specialized models are trained on diverse data sources from both human and AI feedback, enabling accurate comparison and ranking of model responses.</p> <p>To run inference with evaluation models, first install extra dependencies by running:</p>"},{"location":"evaluations/#pairrm","title":"PairRM","text":"<p>Pairwise Reward Model for LLM (PairRM) is an evaluation model specifically designed to assess and compare pairwise responses from LLMs. The model uses DeBERTa (He et al., 2021) as the base model, trained and evaluated on MixInstruct dataset. It is a very lightweight model with high accuracy on par with GPT-4.</p> Model Parameter Size PairRM 0.4B 1.7GB <p>To run evaluation for pairwise ranking:</p> <pre><code>from openpo import PairRM\n\npairrm = PairRM()\nres = pairrm.eval(prompts, responses)\n</code></pre> <p>For full example of how to use PairRM, check out the tutorial notebook!</p>"},{"location":"evaluations/#prometheus-2","title":"Prometheus 2","text":"<p>Prometheus 2 is an open source language model specialized in evaluations of other language models. It is aimed to solve shortcomings of existing proprietary models such as transparency, controllability and affordability.</p> <p>The model uses Mistral-7B and Mixtral-8x7B as the base models, and uses two types of datasets: preference collection and feedback collection to train models on both direct assessment and pairwise ranking.</p> Model Parameter Size prometheus-7b-v2.0 7.2B 13GB prometheus-8x7b-v2.0 46.7B 90GB <p>For pairwise ranking:</p> <pre><code>from openpo import Prometheus2\n\npm = Prometheus2(model=\"prometheus-eval/prometheus-7b-v2.0\")\n\nfeedback = pm.eval_relative(\n    instructions=instructions,\n    responses_A=response_A,\n    responses_B=response_B,\n    rubric='reasoning',\n)\n</code></pre> <p>For direct assessment:</p> <p><pre><code>pm = Prometheus2(model=\"prometheus-eval/prometheus-7b-v2.0\")\n\nfeedback = pm.eval_absolute(\n    instructions=instructions,\n    responses=responses,\n    rubric='reasoning',\n)\n</code></pre> You can choose different rubric depending on the evaluation goal. Prometheus2 supports five rubrics out of the box.</p> <p>OpenPO also lets you use customized rubric:</p> <pre><code>feedback = pm.eval_absolute(\n    instructions=instructions,\n    responses=responses,\n    rubric='some-customized-rubric-template',\n)\n</code></pre> <p>For full example of how to use Prometheus 2, check out the tutorial notebook!</p> <p>For more information about the model, refer to their github repository</p>"},{"location":"installation/","title":"Installation","text":"<p>OpenPO is published as a Python package and can be installed using <code>pip</code>. Simplest way to start is running the following command.</p> <pre><code>pip install openpo\n</code></pre> <p>This will automatically install the latest version and all of its root dependency.</p> <p> To use vLLM for local inference, install extra dependencies by running:</p> <pre><code>pip install openpo[vllm]\n</code></pre> <p> For evaluation models, install dependencies by running:</p> <pre><code>pip install openpo[eval]\n</code></pre>"},{"location":"json/","title":"Structured Output (JSON)","text":"<p>To use structured output, use Pydantic model.</p> <pre><code>from pydantic import BaseModel\nfrom openpo import OpenPO\n\nopenpo = OpenPO()\n\nclass ResponseModel(BaseModel):\n    response: str\n\n\nres = openpo.completion.generate(\n    model=\"huggingface/Qwen/Qwen2.5-Coder-32B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": PROMPT},\n        {\"role\": \"system\", \"content\": MESSAGE},\n    ],\n    params = {\n        \"response_format\": ResponseModel,\n    }\n)\n</code></pre> <p>For more information on Hugging Face's implementation of structured output, refer here</p> <p>Warning</p> <p>OpenRouter does not natively support structured output. This makes the response inconsistent and produces error. Some smaller models on Hugging Face are also prone to inconsistency when structured output is used.</p>"},{"location":"notebook/","title":"Notebooks","text":"<p>Try free notebooks on Google Colab to see how to build datasets using OpenPO!</p> Resources Notebooks Building dataset with OpenPO and PairRM \ud83d\udcd4 Notebook Using OpenPO with Prometheus 2 \ud83d\udcd4 Notebook Evaluating with LLM-as-a-Judge \ud83d\udcd4 Notebook Building dataset using vLLM \ud83d\udcd4 Notebook"},{"location":"paper/","title":"Papers","text":"<p>Recent advances in LLM capabilities have led to breakthrough research in synthetic data generation and automated evaluation methods. Several key studies highlight the potential of using LLMs for tasks traditionally requiring human annotators.</p> <p>ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks demonstrates ChatGPT's capabilities for outperforming crowd workers:</p> <ul> <li>Shows consistently higher accuracy across multiple annotation tasks</li> <li>Suggests viable alternatives to traditional human feedback collection</li> </ul> <p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena validates potential for LLM-as-a-Judge as a foundational evaluation technique:</p> <ul> <li>Shows strong LLMs achieving 80%+ agreement rates, comparable to human experts</li> <li>Provides framework for systematic LLM evaluation</li> </ul> <p>From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge dives deeper into LLM-as-a-Judge paradigm and its broader applications. The paper concludes:</p> <ul> <li>LLMs can effectively assess various attributes, including helpfulness, harmlessness, reliability, relevance, feasibility, and overall quality.</li> <li>Establishes LLM-as-a-Judge as a promising automation paradigm</li> </ul> <p>While challenges like bias and potential errors still exist, these studies suggest LLM-based evaluation methods could effectively reduce dependency on labor-intensive human annotation while maintaining high quality standards.</p>"},{"location":"parameters/","title":"Parameters","text":"<p>Following is available model parameters for completion.</p>"},{"location":"parameters/#completion-using-api","title":"Completion using API","text":""},{"location":"parameters/#required-parameters","title":"Required Parameters","text":"<p>model (str, list[str])<pre><code>* Required\n</code></pre> Specifies the model to use for text generation. Can be a HuggingFace or OpenRouter model</p> <p></p> <p>messages (list[dict])<pre><code>* Required\n</code></pre> messages to send to model. OpenAI format required: <code>{\"role\", \"content\"}</code></p>"},{"location":"parameters/#optional-parameters","title":"Optional Parameters","text":"<p>frequency_penalty (float)<pre><code>* Optional, -2.0 to 2.0\n* Default = 0.0\n</code></pre> Reduces repetitive text by applying a penalty to tokens based on how frequently they've appeared in the generated text.</p> <p></p> <p>logit_bias (list[float])<pre><code>* Optional, -100 to 100\n* Default = None\n</code></pre> Modifies token generation probability by directly adjusting their logit scores. Useful for controlling specific token appearances.</p> <p></p> <p>logprobs (bool)<pre><code>* Optional\n* Default = False\n</code></pre> Enables the return of log probabilities for generated tokens, useful for analyzing model confidence.</p> <p></p> <p>max_tokens (int)<pre><code>* Optional\n* Default = 20\n</code></pre> Limits the length of the model's response by setting a maximum token count.</p> <p></p> <p>presence_penalty (float)<pre><code>* Optional, -2.0 to 2.0\n* Default = None\n</code></pre> Influences topic diversity by penalizing tokens based on their presence in the text so far.</p> <p></p> <p>response_format (PydanticModel)<pre><code>* Optional\n</code></pre> Defines structural constraints for the output. Pydantic model is required.</p> <p></p> <p>seed (int)<pre><code>* Optional\n* Default = None\n</code></pre> Enables reproducible outputs by setting a fixed random seed for generation.</p> <p></p> <p>stop (str)<pre><code>* Optional\n* Default = None\n</code></pre> Defines up to 4 sequences that will cause the model to stop generating further tokens when encountered.</p> <p></p> <p>temperature (float)<pre><code>* Optional, 0.0 to 2.0\n* Default = 1.0\n</code></pre> Controls response randomness - lower values produce more focused and deterministic outputs, while higher values increase creativity.</p> <p></p> <p>top_logprobs (int)<pre><code>* Optional, 0 to 5\n</code></pre> Returns the most probable token alternatives at each position with their probabilities when logprobs is enabled.</p> <p></p> <p>top_p (float)<pre><code>* Optional, 0.0 to 1.0\n* Default = 1.0\n</code></pre> Controls response diversity by sampling from the most likely tokens that sum to the specified probability.</p> <p></p> <p>tool_choice (str)<pre><code>* Optional\n* Default = \"auto\"\n</code></pre> Determines which tool the model should use for completion generation.</p> <p></p> <p>tool_prompt (str)<pre><code>* Optional\n* Only available for HuggingFace models\n</code></pre> Provides additional context or instructions to be prepended before tool-related prompts.</p> <p></p> <p>tools (list)<pre><code>* Optional\n</code></pre> A list of functions that the model may use to generate structured outputs</p> <p></p> <p>pref_params (list[dict[str, float]])<pre><code>* Optional\n</code></pre> Extra parameters to control the model outputs from the model list except the first one . Currently supports <code>temperature</code> and <code>frequency_penalty</code></p> <p></p>"},{"location":"parameters/#completion-using-vllm","title":"Completion using vLLM","text":"<p>Below is the list of parameters for vLLM configuration and its model.</p>"},{"location":"parameters/#llm-engine","title":"LLM Engine","text":"<p>LLM configuration</p>"},{"location":"parameters/#chat-parameter","title":"Chat Parameter","text":"<p>Chat</p>"},{"location":"parameters/#sampling-parameters","title":"Sampling Parameters","text":"<p>Sampling Parameters</p>"},{"location":"provider/","title":"Providers","text":"<p>List of providers OpenPO currently supports.</p>"},{"location":"provider/#for-completion","title":"For Completion","text":"<p>Completion supports the models from the following providers:</p> Provider Name HuggingFace <code>huggingface</code> OpenRouter <code>openrouter</code>"},{"location":"provider/#vllm","title":"vLLM","text":"<p>List of all supported model by vLLM can be found here.</p>"},{"location":"provider/#for-evaluations","title":"For Evaluations","text":"<p>LLM-as-a-judge evaluation supports models from the following providers:</p> Provider Name OpenAI <code>openai</code> Anthropic <code>anthropic</code>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#set-your-api-key","title":"Set your API Key","text":"<p>Configure your API key as an environment variable. You can also pass the key into the client.</p> <pre><code># for completion\nexport HF_API_KEY=&lt;your-huggingface-api-key&gt;\nexport OPENROUTER_API_KEY=&lt;your-openrouter-api-key&gt;\n\n# for evaluation\nexport OPENAI_API_KEY=&lt;your-openai-api-key&gt;\nexport ANTHROPIC_API_KEY=&lt;your-anthropic-api-key&gt;\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>model</code> parameter accepts a model identifier or list of model identifiers.</p> <pre><code>from openpo import OpenPO\n\nclient = OpenPO()\n\n# use single model\nresponse = client.completion.generate(\n    model=\"huggingface/mistralai/Mistral-7B-Instruct-v0.3\",\n    messages=[\n        {\"role\": \"system\", \"content\": PROMPT},\n        {\"role\": \"user\", \"content\": MESSAGE},\n    ]\n)\n\n# use multiple models\nresponse = client.completion.generate(\n    model=[\n        \"huggingface/mistralai/Mistral-7B-Instruct-v0.3\",\n        \"huggingface/microsoft/Phi-3.5-mini-instruct\"\n    ]\n    messages=[\n        {\"role\": \"system\", \"content\": PROMPT},\n        {\"role\": \"user\", \"content\": MESSAGE},\n    ]\n)\n</code></pre> <p>Use with OpenRouter models: <pre><code>client = OpenPO()\n\nresponse = client.completion.generate(\n    model=[\n        \"openrouter/mistralai/mistral-7b-instruct-v0.3\",\n        \"openrouter/microsoft/phi-3.5-mini-128k-instruct\",\n    ],\n    messages=[\n        {\"role\": \"system\", \"content\": PROMPT},\n        {\"role\": \"user\", \"content\": MESSAGE},\n    ],\n)\n</code></pre></p>"},{"location":"quickstart/#inference-endpoint","title":"Inference Endpoint","text":"<p>Call models deployed on HuggingFace Inference Endpoint</p> <pre><code>response = client.completion.generate(\n    model=[\n        'huggingface/your-inference-endpoint-1',\n        'huggingface/your-inference-endpoint-2',\n    ],\n    messages=[\n        {\"role\": \"system\", \"content\": PROMPT},\n        {\"role\": \"user\", \"content\": MESSAGE},\n    ],\n)\n</code></pre>"},{"location":"response/","title":"Responses","text":"<p>OpenPO returns a custom response from completions that is compatible with OpenAI chat completion response.</p>"},{"location":"response/#chatcompletionoutput","title":"ChatCompletionOutput","text":"<pre><code>class ChatCompletionOutput:\n    id: str,\n    provider: str,\n    model: str,\n    object: 'chat.completion' | 'chat.completion.chunk',\n    created: int,\n    choices: ChatCompletionOutputComplete,\n    usage: ChatCompletionOutputUsage\n</code></pre> <pre><code>class ChatCompletionOutputComplete\n    finish_reason: str,\n    index: int,\n    message: ChatCompletionOutputMessage,\n    lobprobs: bool,\n</code></pre> <pre><code>class ChatCompletionOutputUsage:\n    completion_tokens: int\n    prompt_tokens: int\n    total_tokens: int\n</code></pre>"},{"location":"storage/","title":"Storage Providers","text":"<p>OpenPO provides storage class for S3 and HuggingFace Dataset repository out of the box. Use storage class to easily upload and download datasets.</p>"},{"location":"storage/#huggingface-storage","title":"HuggingFace Storage","text":"<p><code>HuggingFaceStorage</code> class supports python object and pandas DataFrame as input data types. To use HuggingFace as your datastore:</p> <pre><code>from openpo.storage import HuggingFaceStorage\n\nhf_storage = HuggingFaceStorage(api_key=\"hf-token\") # api_key can also be set as environment variable.\n\n# push data to repo\npreference = [{\"prompt\": \"text\", \"preferred\": \"response1\", \"rejected\": \"response2\"}]\nhf_storage.push_to_repo(repo_id=\"my-hf-repo\", data=preference)\n\n# Load data from repo\ndata = hf_storage.load_from_repo(path=\"my-hf-repo\")\n</code></pre>"},{"location":"storage/#s3-storage","title":"S3 Storage","text":"<p><code>S3Storage</code> supports serialization for <code>json</code> and <code>parquet</code>. To initialize the class, you can either pass in the keyword arguments or configure aws credentials with <code>aws configure</code></p> <pre><code>from openpo.storage import S3Storage\n\ns3 = S3Storage(\n    region_name=\"us-west-2\",              # Optional: AWS region\n    aws_access_key_id=\"access_key\",       # Optional: AWS access key\n    aws_secret_access_key=\"secret_key\",   # Optional: AWS secret key\n    profile_name=\"default\"                # Optional: AWS profile name\n)\n\n# push data to s3\npreference = {\"prompt\": \"text\", \"preferred\": \"response1\", \"rejected\": \"response2\"}\ns3.push_to_s3(\n    data=preference,\n    bucket=\"my-bucket\",\n    key=\"my-key\",\n    ext_type='parquet',\n)\n\n# load data from s3\ndata = s3.load_from_s3(bucket='my-bucket', key='data-key')\n</code></pre>"}]}